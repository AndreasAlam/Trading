{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define 14 tickers to use\n",
    "tickers_list = ['AAPL', 'MSFT', 'WMT', 'IBM', 'MU', 'BA', 'AXP',  'NVDA', 'FB', 'OXY', 'TSLA', 'JNJ', 'NFLX', 'DIS']\n",
    "from_date, to_date = '2014-03-01','2019-03-12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  14 of 14 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AXP</th>\n",
       "      <th>BA</th>\n",
       "      <th>DIS</th>\n",
       "      <th>FB</th>\n",
       "      <th>IBM</th>\n",
       "      <th>JNJ</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>MU</th>\n",
       "      <th>NFLX</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>OXY</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>WMT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-02-28</th>\n",
       "      <td>67.710892</td>\n",
       "      <td>82.985603</td>\n",
       "      <td>109.956337</td>\n",
       "      <td>74.370750</td>\n",
       "      <td>68.459999</td>\n",
       "      <td>143.043030</td>\n",
       "      <td>77.661484</td>\n",
       "      <td>33.542198</td>\n",
       "      <td>24.190001</td>\n",
       "      <td>63.661430</td>\n",
       "      <td>17.476261</td>\n",
       "      <td>68.598862</td>\n",
       "      <td>244.809998</td>\n",
       "      <td>63.742687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-03-03</th>\n",
       "      <td>67.906464</td>\n",
       "      <td>81.794632</td>\n",
       "      <td>109.359322</td>\n",
       "      <td>73.128334</td>\n",
       "      <td>67.410004</td>\n",
       "      <td>142.340057</td>\n",
       "      <td>77.189392</td>\n",
       "      <td>33.078148</td>\n",
       "      <td>24.480000</td>\n",
       "      <td>63.655716</td>\n",
       "      <td>17.390688</td>\n",
       "      <td>68.264824</td>\n",
       "      <td>250.559998</td>\n",
       "      <td>63.247753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-03-04</th>\n",
       "      <td>68.354248</td>\n",
       "      <td>84.194740</td>\n",
       "      <td>111.073647</td>\n",
       "      <td>75.199028</td>\n",
       "      <td>68.800003</td>\n",
       "      <td>144.024139</td>\n",
       "      <td>78.690002</td>\n",
       "      <td>33.629753</td>\n",
       "      <td>25.110001</td>\n",
       "      <td>64.997147</td>\n",
       "      <td>17.571339</td>\n",
       "      <td>69.096352</td>\n",
       "      <td>254.839996</td>\n",
       "      <td>64.109596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-03-05</th>\n",
       "      <td>68.498352</td>\n",
       "      <td>83.767448</td>\n",
       "      <td>109.845436</td>\n",
       "      <td>76.091743</td>\n",
       "      <td>71.570000</td>\n",
       "      <td>144.564880</td>\n",
       "      <td>78.057709</td>\n",
       "      <td>33.367085</td>\n",
       "      <td>24.830000</td>\n",
       "      <td>64.785713</td>\n",
       "      <td>17.723475</td>\n",
       "      <td>68.833405</td>\n",
       "      <td>252.660004</td>\n",
       "      <td>63.828003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-03-06</th>\n",
       "      <td>68.291168</td>\n",
       "      <td>85.022049</td>\n",
       "      <td>109.905167</td>\n",
       "      <td>76.699165</td>\n",
       "      <td>70.839996</td>\n",
       "      <td>144.951080</td>\n",
       "      <td>78.310631</td>\n",
       "      <td>33.402107</td>\n",
       "      <td>24.770000</td>\n",
       "      <td>64.358574</td>\n",
       "      <td>17.476261</td>\n",
       "      <td>68.933640</td>\n",
       "      <td>252.940002</td>\n",
       "      <td>63.896255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 AAPL        AXP          BA        DIS         FB  \\\n",
       "Date                                                                 \n",
       "2014-02-28  67.710892  82.985603  109.956337  74.370750  68.459999   \n",
       "2014-03-03  67.906464  81.794632  109.359322  73.128334  67.410004   \n",
       "2014-03-04  68.354248  84.194740  111.073647  75.199028  68.800003   \n",
       "2014-03-05  68.498352  83.767448  109.845436  76.091743  71.570000   \n",
       "2014-03-06  68.291168  85.022049  109.905167  76.699165  70.839996   \n",
       "\n",
       "                   IBM        JNJ       MSFT         MU       NFLX       NVDA  \\\n",
       "Date                                                                            \n",
       "2014-02-28  143.043030  77.661484  33.542198  24.190001  63.661430  17.476261   \n",
       "2014-03-03  142.340057  77.189392  33.078148  24.480000  63.655716  17.390688   \n",
       "2014-03-04  144.024139  78.690002  33.629753  25.110001  64.997147  17.571339   \n",
       "2014-03-05  144.564880  78.057709  33.367085  24.830000  64.785713  17.723475   \n",
       "2014-03-06  144.951080  78.310631  33.402107  24.770000  64.358574  17.476261   \n",
       "\n",
       "                  OXY        TSLA        WMT  \n",
       "Date                                          \n",
       "2014-02-28  68.598862  244.809998  63.742687  \n",
       "2014-03-03  68.264824  250.559998  63.247753  \n",
       "2014-03-04  69.096352  254.839996  64.109596  \n",
       "2014-03-05  68.833405  252.660004  63.828003  \n",
       "2014-03-06  68.933640  252.940002  63.896255  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_close_prices(tick_list, from_date, to_date):\n",
    "    return yf.download(tickers_list, from_date, to_date )['Adj Close']\n",
    "    \n",
    "data_set_df = get_close_prices(tickers_list, from_date, to_date )\n",
    "data_set_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_df = pd.concat([data_set_df, pd.DataFrame( columns = [x + '_Signal' for x in tickers_list] )]).reset_index()\n",
    "\n",
    "window = 11\n",
    "for ticker in tickers_list:\n",
    "    for idx in range(dataset_df.shape[0]):\n",
    "        if int(idx)>window:\n",
    "            #print(type(idx))\n",
    "            start_idx = idx - window\n",
    "            end_idx = idx\n",
    "            prices = dataset_df[ticker].iloc[start_idx:end_idx]\n",
    "            max_idx = max(enumerate(prices.tolist()),key=lambda x: x[1])[0]\n",
    "            min_idx =  min(enumerate(prices.tolist()),key=lambda x: x[1])[0]        \n",
    "            pos = int(end_idx-6)\n",
    "            if max_idx == window//2:\n",
    "                dataset_df.at[pos, [f'{ticker}_Signal']]  = 'Sell'         \n",
    "            elif min_idx == window//2:\n",
    "                dataset_df.at[pos, [f'{ticker}_Signal']]  = 'Buy'        \n",
    "            else:\n",
    "                dataset_df.at[pos, [f'{ticker}_Signal']] = 'Hold'\n",
    "\n",
    "dataset_df = dataset_df.reindex(sorted(dataset_df.columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = dataset_df.set_index('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = dataset_df[11:-6]##.to_csv('C:\\\\Users\\\\andre\\\\Desktop\\\\test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AAPL_Signal</th>\n",
       "      <th>AXP</th>\n",
       "      <th>AXP_Signal</th>\n",
       "      <th>BA</th>\n",
       "      <th>BA_Signal</th>\n",
       "      <th>DIS</th>\n",
       "      <th>DIS_Signal</th>\n",
       "      <th>FB</th>\n",
       "      <th>FB_Signal</th>\n",
       "      <th>...</th>\n",
       "      <th>NFLX</th>\n",
       "      <th>NFLX_Signal</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>NVDA_Signal</th>\n",
       "      <th>OXY</th>\n",
       "      <th>OXY_Signal</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>TSLA_Signal</th>\n",
       "      <th>WMT</th>\n",
       "      <th>WMT_Signal</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-03-17</th>\n",
       "      <td>67.775238</td>\n",
       "      <td>Hold</td>\n",
       "      <td>82.967430</td>\n",
       "      <td>Hold</td>\n",
       "      <td>106.971184</td>\n",
       "      <td>Hold</td>\n",
       "      <td>74.904541</td>\n",
       "      <td>Hold</td>\n",
       "      <td>68.739998</td>\n",
       "      <td>Hold</td>\n",
       "      <td>...</td>\n",
       "      <td>60.388573</td>\n",
       "      <td>Hold</td>\n",
       "      <td>16.943790</td>\n",
       "      <td>Hold</td>\n",
       "      <td>69.299553</td>\n",
       "      <td>Hold</td>\n",
       "      <td>233.979996</td>\n",
       "      <td>Hold</td>\n",
       "      <td>64.136726</td>\n",
       "      <td>Hold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-03-18</th>\n",
       "      <td>68.374832</td>\n",
       "      <td>Hold</td>\n",
       "      <td>83.267441</td>\n",
       "      <td>Hold</td>\n",
       "      <td>105.794182</td>\n",
       "      <td>Hold</td>\n",
       "      <td>75.456726</td>\n",
       "      <td>Sell</td>\n",
       "      <td>69.190002</td>\n",
       "      <td>Hold</td>\n",
       "      <td>...</td>\n",
       "      <td>60.035713</td>\n",
       "      <td>Hold</td>\n",
       "      <td>17.343140</td>\n",
       "      <td>Hold</td>\n",
       "      <td>69.592934</td>\n",
       "      <td>Sell</td>\n",
       "      <td>240.039993</td>\n",
       "      <td>Hold</td>\n",
       "      <td>64.214005</td>\n",
       "      <td>Hold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-03-19</th>\n",
       "      <td>68.356827</td>\n",
       "      <td>Hold</td>\n",
       "      <td>82.485565</td>\n",
       "      <td>Hold</td>\n",
       "      <td>104.258942</td>\n",
       "      <td>Hold</td>\n",
       "      <td>74.103859</td>\n",
       "      <td>Hold</td>\n",
       "      <td>68.239998</td>\n",
       "      <td>Hold</td>\n",
       "      <td>...</td>\n",
       "      <td>60.012856</td>\n",
       "      <td>Hold</td>\n",
       "      <td>17.647408</td>\n",
       "      <td>Hold</td>\n",
       "      <td>68.075989</td>\n",
       "      <td>Hold</td>\n",
       "      <td>235.839996</td>\n",
       "      <td>Hold</td>\n",
       "      <td>63.879086</td>\n",
       "      <td>Hold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-03-20</th>\n",
       "      <td>68.027428</td>\n",
       "      <td>Hold</td>\n",
       "      <td>83.358345</td>\n",
       "      <td>Sell</td>\n",
       "      <td>105.529762</td>\n",
       "      <td>Hold</td>\n",
       "      <td>74.370750</td>\n",
       "      <td>Hold</td>\n",
       "      <td>66.970001</td>\n",
       "      <td>Hold</td>\n",
       "      <td>...</td>\n",
       "      <td>60.610001</td>\n",
       "      <td>Hold</td>\n",
       "      <td>17.656912</td>\n",
       "      <td>Sell</td>\n",
       "      <td>66.494667</td>\n",
       "      <td>Buy</td>\n",
       "      <td>234.910004</td>\n",
       "      <td>Hold</td>\n",
       "      <td>64.737915</td>\n",
       "      <td>Hold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-03-21</th>\n",
       "      <td>68.563950</td>\n",
       "      <td>Hold</td>\n",
       "      <td>83.203766</td>\n",
       "      <td>Hold</td>\n",
       "      <td>104.548927</td>\n",
       "      <td>Hold</td>\n",
       "      <td>73.947418</td>\n",
       "      <td>Hold</td>\n",
       "      <td>67.239998</td>\n",
       "      <td>Hold</td>\n",
       "      <td>...</td>\n",
       "      <td>57.998573</td>\n",
       "      <td>Hold</td>\n",
       "      <td>17.628389</td>\n",
       "      <td>Hold</td>\n",
       "      <td>67.310387</td>\n",
       "      <td>Hold</td>\n",
       "      <td>228.889999</td>\n",
       "      <td>Hold</td>\n",
       "      <td>65.356247</td>\n",
       "      <td>Hold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-03-24</th>\n",
       "      <td>69.377159</td>\n",
       "      <td>Hold</td>\n",
       "      <td>82.740120</td>\n",
       "      <td>Hold</td>\n",
       "      <td>105.265373</td>\n",
       "      <td>Hold</td>\n",
       "      <td>73.155937</td>\n",
       "      <td>Hold</td>\n",
       "      <td>64.099998</td>\n",
       "      <td>Hold</td>\n",
       "      <td>...</td>\n",
       "      <td>54.128571</td>\n",
       "      <td>Hold</td>\n",
       "      <td>17.542820</td>\n",
       "      <td>Hold</td>\n",
       "      <td>67.167274</td>\n",
       "      <td>Hold</td>\n",
       "      <td>220.169998</td>\n",
       "      <td>Hold</td>\n",
       "      <td>65.923088</td>\n",
       "      <td>Hold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-03-25</th>\n",
       "      <td>70.123436</td>\n",
       "      <td>Sell</td>\n",
       "      <td>82.585579</td>\n",
       "      <td>Hold</td>\n",
       "      <td>105.777084</td>\n",
       "      <td>Hold</td>\n",
       "      <td>73.211151</td>\n",
       "      <td>Hold</td>\n",
       "      <td>64.889999</td>\n",
       "      <td>Hold</td>\n",
       "      <td>...</td>\n",
       "      <td>52.977142</td>\n",
       "      <td>Hold</td>\n",
       "      <td>17.542820</td>\n",
       "      <td>Hold</td>\n",
       "      <td>67.725380</td>\n",
       "      <td>Hold</td>\n",
       "      <td>220.440002</td>\n",
       "      <td>Hold</td>\n",
       "      <td>66.017555</td>\n",
       "      <td>Sell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-03-26</th>\n",
       "      <td>69.453064</td>\n",
       "      <td>Hold</td>\n",
       "      <td>81.512794</td>\n",
       "      <td>Buy</td>\n",
       "      <td>105.359184</td>\n",
       "      <td>Hold</td>\n",
       "      <td>72.355255</td>\n",
       "      <td>Hold</td>\n",
       "      <td>60.389999</td>\n",
       "      <td>Hold</td>\n",
       "      <td>...</td>\n",
       "      <td>53.182858</td>\n",
       "      <td>Hold</td>\n",
       "      <td>17.143465</td>\n",
       "      <td>Hold</td>\n",
       "      <td>66.931137</td>\n",
       "      <td>Hold</td>\n",
       "      <td>212.960007</td>\n",
       "      <td>Hold</td>\n",
       "      <td>65.467911</td>\n",
       "      <td>Hold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-03-27</th>\n",
       "      <td>69.154572</td>\n",
       "      <td>Hold</td>\n",
       "      <td>81.812805</td>\n",
       "      <td>Hold</td>\n",
       "      <td>105.086250</td>\n",
       "      <td>Hold</td>\n",
       "      <td>72.226418</td>\n",
       "      <td>Buy</td>\n",
       "      <td>60.970001</td>\n",
       "      <td>Hold</td>\n",
       "      <td>...</td>\n",
       "      <td>52.025715</td>\n",
       "      <td>Hold</td>\n",
       "      <td>16.915268</td>\n",
       "      <td>Buy</td>\n",
       "      <td>67.718246</td>\n",
       "      <td>Hold</td>\n",
       "      <td>207.320007</td>\n",
       "      <td>Buy</td>\n",
       "      <td>65.390625</td>\n",
       "      <td>Hold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-03-28</th>\n",
       "      <td>69.077377</td>\n",
       "      <td>Hold</td>\n",
       "      <td>82.240120</td>\n",
       "      <td>Hold</td>\n",
       "      <td>106.152390</td>\n",
       "      <td>Hold</td>\n",
       "      <td>72.695793</td>\n",
       "      <td>Hold</td>\n",
       "      <td>60.009998</td>\n",
       "      <td>Hold</td>\n",
       "      <td>...</td>\n",
       "      <td>51.267143</td>\n",
       "      <td>Hold</td>\n",
       "      <td>17.019857</td>\n",
       "      <td>Hold</td>\n",
       "      <td>68.369370</td>\n",
       "      <td>Hold</td>\n",
       "      <td>212.369995</td>\n",
       "      <td>Hold</td>\n",
       "      <td>65.278976</td>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-03-31</th>\n",
       "      <td>69.061943</td>\n",
       "      <td>Hold</td>\n",
       "      <td>81.849159</td>\n",
       "      <td>Hold</td>\n",
       "      <td>107.030876</td>\n",
       "      <td>Hold</td>\n",
       "      <td>73.689720</td>\n",
       "      <td>Hold</td>\n",
       "      <td>60.240002</td>\n",
       "      <td>Hold</td>\n",
       "      <td>...</td>\n",
       "      <td>50.290001</td>\n",
       "      <td>Hold</td>\n",
       "      <td>17.029367</td>\n",
       "      <td>Hold</td>\n",
       "      <td>68.183334</td>\n",
       "      <td>Hold</td>\n",
       "      <td>208.449997</td>\n",
       "      <td>Hold</td>\n",
       "      <td>65.639679</td>\n",
       "      <td>Hold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-01</th>\n",
       "      <td>69.693695</td>\n",
       "      <td>Hold</td>\n",
       "      <td>82.885590</td>\n",
       "      <td>Hold</td>\n",
       "      <td>109.350784</td>\n",
       "      <td>Hold</td>\n",
       "      <td>75.070198</td>\n",
       "      <td>Hold</td>\n",
       "      <td>62.619999</td>\n",
       "      <td>Hold</td>\n",
       "      <td>...</td>\n",
       "      <td>52.098572</td>\n",
       "      <td>Hold</td>\n",
       "      <td>17.732981</td>\n",
       "      <td>Hold</td>\n",
       "      <td>67.904274</td>\n",
       "      <td>Hold</td>\n",
       "      <td>216.970001</td>\n",
       "      <td>Hold</td>\n",
       "      <td>65.931664</td>\n",
       "      <td>Hold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-02</th>\n",
       "      <td>69.809486</td>\n",
       "      <td>Sell</td>\n",
       "      <td>82.393410</td>\n",
       "      <td>Hold</td>\n",
       "      <td>109.436073</td>\n",
       "      <td>Hold</td>\n",
       "      <td>75.162224</td>\n",
       "      <td>Hold</td>\n",
       "      <td>62.720001</td>\n",
       "      <td>Sell</td>\n",
       "      <td>...</td>\n",
       "      <td>51.840000</td>\n",
       "      <td>Hold</td>\n",
       "      <td>17.618881</td>\n",
       "      <td>Hold</td>\n",
       "      <td>68.705658</td>\n",
       "      <td>Hold</td>\n",
       "      <td>230.289993</td>\n",
       "      <td>Sell</td>\n",
       "      <td>66.283783</td>\n",
       "      <td>Hold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-03</th>\n",
       "      <td>69.325684</td>\n",
       "      <td>Hold</td>\n",
       "      <td>82.922050</td>\n",
       "      <td>Sell</td>\n",
       "      <td>109.836945</td>\n",
       "      <td>Sell</td>\n",
       "      <td>75.180634</td>\n",
       "      <td>Sell</td>\n",
       "      <td>59.490002</td>\n",
       "      <td>Hold</td>\n",
       "      <td>...</td>\n",
       "      <td>50.669998</td>\n",
       "      <td>Hold</td>\n",
       "      <td>17.809048</td>\n",
       "      <td>Hold</td>\n",
       "      <td>69.056282</td>\n",
       "      <td>Sell</td>\n",
       "      <td>225.399994</td>\n",
       "      <td>Hold</td>\n",
       "      <td>66.524277</td>\n",
       "      <td>Hold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-04</th>\n",
       "      <td>68.428879</td>\n",
       "      <td>Hold</td>\n",
       "      <td>81.272362</td>\n",
       "      <td>Hold</td>\n",
       "      <td>108.642860</td>\n",
       "      <td>Hold</td>\n",
       "      <td>74.021027</td>\n",
       "      <td>Hold</td>\n",
       "      <td>56.750000</td>\n",
       "      <td>Buy</td>\n",
       "      <td>...</td>\n",
       "      <td>48.187141</td>\n",
       "      <td>Hold</td>\n",
       "      <td>17.257561</td>\n",
       "      <td>Hold</td>\n",
       "      <td>67.653831</td>\n",
       "      <td>Hold</td>\n",
       "      <td>212.229996</td>\n",
       "      <td>Hold</td>\n",
       "      <td>66.395416</td>\n",
       "      <td>Hold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-07</th>\n",
       "      <td>67.354500</td>\n",
       "      <td>Hold</td>\n",
       "      <td>78.929985</td>\n",
       "      <td>Hold</td>\n",
       "      <td>107.116158</td>\n",
       "      <td>Hold</td>\n",
       "      <td>72.824615</td>\n",
       "      <td>Hold</td>\n",
       "      <td>56.950001</td>\n",
       "      <td>Hold</td>\n",
       "      <td>...</td>\n",
       "      <td>48.285713</td>\n",
       "      <td>Hold</td>\n",
       "      <td>17.314615</td>\n",
       "      <td>Hold</td>\n",
       "      <td>66.966927</td>\n",
       "      <td>Hold</td>\n",
       "      <td>207.520004</td>\n",
       "      <td>Hold</td>\n",
       "      <td>66.395416</td>\n",
       "      <td>Hold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-08</th>\n",
       "      <td>67.350624</td>\n",
       "      <td>Hold</td>\n",
       "      <td>78.829720</td>\n",
       "      <td>Hold</td>\n",
       "      <td>105.888008</td>\n",
       "      <td>Hold</td>\n",
       "      <td>73.229568</td>\n",
       "      <td>Hold</td>\n",
       "      <td>58.189999</td>\n",
       "      <td>Hold</td>\n",
       "      <td>...</td>\n",
       "      <td>49.841427</td>\n",
       "      <td>Hold</td>\n",
       "      <td>17.932653</td>\n",
       "      <td>Sell</td>\n",
       "      <td>67.940056</td>\n",
       "      <td>Hold</td>\n",
       "      <td>215.460007</td>\n",
       "      <td>Hold</td>\n",
       "      <td>67.142616</td>\n",
       "      <td>Sell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-09</th>\n",
       "      <td>68.235886</td>\n",
       "      <td>Hold</td>\n",
       "      <td>80.862206</td>\n",
       "      <td>Hold</td>\n",
       "      <td>108.216415</td>\n",
       "      <td>Hold</td>\n",
       "      <td>74.057846</td>\n",
       "      <td>Hold</td>\n",
       "      <td>62.410000</td>\n",
       "      <td>Hold</td>\n",
       "      <td>...</td>\n",
       "      <td>50.432858</td>\n",
       "      <td>Hold</td>\n",
       "      <td>17.913637</td>\n",
       "      <td>Hold</td>\n",
       "      <td>68.669891</td>\n",
       "      <td>Hold</td>\n",
       "      <td>216.929993</td>\n",
       "      <td>Hold</td>\n",
       "      <td>66.962257</td>\n",
       "      <td>Hold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-10</th>\n",
       "      <td>67.355766</td>\n",
       "      <td>Hold</td>\n",
       "      <td>77.799789</td>\n",
       "      <td>Hold</td>\n",
       "      <td>105.453026</td>\n",
       "      <td>Hold</td>\n",
       "      <td>71.333710</td>\n",
       "      <td>Hold</td>\n",
       "      <td>59.160000</td>\n",
       "      <td>Hold</td>\n",
       "      <td>...</td>\n",
       "      <td>47.818573</td>\n",
       "      <td>Hold</td>\n",
       "      <td>17.476261</td>\n",
       "      <td>Hold</td>\n",
       "      <td>67.575096</td>\n",
       "      <td>Hold</td>\n",
       "      <td>204.190002</td>\n",
       "      <td>Hold</td>\n",
       "      <td>66.034698</td>\n",
       "      <td>Hold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-11</th>\n",
       "      <td>66.857819</td>\n",
       "      <td>Hold</td>\n",
       "      <td>77.052422</td>\n",
       "      <td>Buy</td>\n",
       "      <td>104.113930</td>\n",
       "      <td>Buy</td>\n",
       "      <td>70.873543</td>\n",
       "      <td>Buy</td>\n",
       "      <td>58.529999</td>\n",
       "      <td>Hold</td>\n",
       "      <td>...</td>\n",
       "      <td>46.672855</td>\n",
       "      <td>Hold</td>\n",
       "      <td>17.229044</td>\n",
       "      <td>Buy</td>\n",
       "      <td>66.780869</td>\n",
       "      <td>Buy</td>\n",
       "      <td>203.779999</td>\n",
       "      <td>Hold</td>\n",
       "      <td>65.699791</td>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 AAPL AAPL_Signal        AXP AXP_Signal          BA BA_Signal  \\\n",
       "index                                                                           \n",
       "2014-03-17  67.775238        Hold  82.967430       Hold  106.971184      Hold   \n",
       "2014-03-18  68.374832        Hold  83.267441       Hold  105.794182      Hold   \n",
       "2014-03-19  68.356827        Hold  82.485565       Hold  104.258942      Hold   \n",
       "2014-03-20  68.027428        Hold  83.358345       Sell  105.529762      Hold   \n",
       "2014-03-21  68.563950        Hold  83.203766       Hold  104.548927      Hold   \n",
       "2014-03-24  69.377159        Hold  82.740120       Hold  105.265373      Hold   \n",
       "2014-03-25  70.123436        Sell  82.585579       Hold  105.777084      Hold   \n",
       "2014-03-26  69.453064        Hold  81.512794        Buy  105.359184      Hold   \n",
       "2014-03-27  69.154572        Hold  81.812805       Hold  105.086250      Hold   \n",
       "2014-03-28  69.077377        Hold  82.240120       Hold  106.152390      Hold   \n",
       "2014-03-31  69.061943        Hold  81.849159       Hold  107.030876      Hold   \n",
       "2014-04-01  69.693695        Hold  82.885590       Hold  109.350784      Hold   \n",
       "2014-04-02  69.809486        Sell  82.393410       Hold  109.436073      Hold   \n",
       "2014-04-03  69.325684        Hold  82.922050       Sell  109.836945      Sell   \n",
       "2014-04-04  68.428879        Hold  81.272362       Hold  108.642860      Hold   \n",
       "2014-04-07  67.354500        Hold  78.929985       Hold  107.116158      Hold   \n",
       "2014-04-08  67.350624        Hold  78.829720       Hold  105.888008      Hold   \n",
       "2014-04-09  68.235886        Hold  80.862206       Hold  108.216415      Hold   \n",
       "2014-04-10  67.355766        Hold  77.799789       Hold  105.453026      Hold   \n",
       "2014-04-11  66.857819        Hold  77.052422        Buy  104.113930       Buy   \n",
       "\n",
       "                  DIS DIS_Signal         FB FB_Signal  ...       NFLX  \\\n",
       "index                                                  ...              \n",
       "2014-03-17  74.904541       Hold  68.739998      Hold  ...  60.388573   \n",
       "2014-03-18  75.456726       Sell  69.190002      Hold  ...  60.035713   \n",
       "2014-03-19  74.103859       Hold  68.239998      Hold  ...  60.012856   \n",
       "2014-03-20  74.370750       Hold  66.970001      Hold  ...  60.610001   \n",
       "2014-03-21  73.947418       Hold  67.239998      Hold  ...  57.998573   \n",
       "2014-03-24  73.155937       Hold  64.099998      Hold  ...  54.128571   \n",
       "2014-03-25  73.211151       Hold  64.889999      Hold  ...  52.977142   \n",
       "2014-03-26  72.355255       Hold  60.389999      Hold  ...  53.182858   \n",
       "2014-03-27  72.226418        Buy  60.970001      Hold  ...  52.025715   \n",
       "2014-03-28  72.695793       Hold  60.009998      Hold  ...  51.267143   \n",
       "2014-03-31  73.689720       Hold  60.240002      Hold  ...  50.290001   \n",
       "2014-04-01  75.070198       Hold  62.619999      Hold  ...  52.098572   \n",
       "2014-04-02  75.162224       Hold  62.720001      Sell  ...  51.840000   \n",
       "2014-04-03  75.180634       Sell  59.490002      Hold  ...  50.669998   \n",
       "2014-04-04  74.021027       Hold  56.750000       Buy  ...  48.187141   \n",
       "2014-04-07  72.824615       Hold  56.950001      Hold  ...  48.285713   \n",
       "2014-04-08  73.229568       Hold  58.189999      Hold  ...  49.841427   \n",
       "2014-04-09  74.057846       Hold  62.410000      Hold  ...  50.432858   \n",
       "2014-04-10  71.333710       Hold  59.160000      Hold  ...  47.818573   \n",
       "2014-04-11  70.873543        Buy  58.529999      Hold  ...  46.672855   \n",
       "\n",
       "           NFLX_Signal       NVDA NVDA_Signal        OXY OXY_Signal  \\\n",
       "index                                                                 \n",
       "2014-03-17        Hold  16.943790        Hold  69.299553       Hold   \n",
       "2014-03-18        Hold  17.343140        Hold  69.592934       Sell   \n",
       "2014-03-19        Hold  17.647408        Hold  68.075989       Hold   \n",
       "2014-03-20        Hold  17.656912        Sell  66.494667        Buy   \n",
       "2014-03-21        Hold  17.628389        Hold  67.310387       Hold   \n",
       "2014-03-24        Hold  17.542820        Hold  67.167274       Hold   \n",
       "2014-03-25        Hold  17.542820        Hold  67.725380       Hold   \n",
       "2014-03-26        Hold  17.143465        Hold  66.931137       Hold   \n",
       "2014-03-27        Hold  16.915268         Buy  67.718246       Hold   \n",
       "2014-03-28        Hold  17.019857        Hold  68.369370       Hold   \n",
       "2014-03-31        Hold  17.029367        Hold  68.183334       Hold   \n",
       "2014-04-01        Hold  17.732981        Hold  67.904274       Hold   \n",
       "2014-04-02        Hold  17.618881        Hold  68.705658       Hold   \n",
       "2014-04-03        Hold  17.809048        Hold  69.056282       Sell   \n",
       "2014-04-04        Hold  17.257561        Hold  67.653831       Hold   \n",
       "2014-04-07        Hold  17.314615        Hold  66.966927       Hold   \n",
       "2014-04-08        Hold  17.932653        Sell  67.940056       Hold   \n",
       "2014-04-09        Hold  17.913637        Hold  68.669891       Hold   \n",
       "2014-04-10        Hold  17.476261        Hold  67.575096       Hold   \n",
       "2014-04-11        Hold  17.229044         Buy  66.780869        Buy   \n",
       "\n",
       "                  TSLA TSLA_Signal        WMT WMT_Signal  \n",
       "index                                                     \n",
       "2014-03-17  233.979996        Hold  64.136726       Hold  \n",
       "2014-03-18  240.039993        Hold  64.214005       Hold  \n",
       "2014-03-19  235.839996        Hold  63.879086       Hold  \n",
       "2014-03-20  234.910004        Hold  64.737915       Hold  \n",
       "2014-03-21  228.889999        Hold  65.356247       Hold  \n",
       "2014-03-24  220.169998        Hold  65.923088       Hold  \n",
       "2014-03-25  220.440002        Hold  66.017555       Sell  \n",
       "2014-03-26  212.960007        Hold  65.467911       Hold  \n",
       "2014-03-27  207.320007         Buy  65.390625       Hold  \n",
       "2014-03-28  212.369995        Hold  65.278976        Buy  \n",
       "2014-03-31  208.449997        Hold  65.639679       Hold  \n",
       "2014-04-01  216.970001        Hold  65.931664       Hold  \n",
       "2014-04-02  230.289993        Sell  66.283783       Hold  \n",
       "2014-04-03  225.399994        Hold  66.524277       Hold  \n",
       "2014-04-04  212.229996        Hold  66.395416       Hold  \n",
       "2014-04-07  207.520004        Hold  66.395416       Hold  \n",
       "2014-04-08  215.460007        Hold  67.142616       Sell  \n",
       "2014-04-09  216.929993        Hold  66.962257       Hold  \n",
       "2014-04-10  204.190002        Hold  66.034698       Hold  \n",
       "2014-04-11  203.779999        Hold  65.699791        Buy  \n",
       "\n",
       "[20 rows x 28 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EMA_features(df, col, short_win =5, long_win = 12 ):\n",
    "        return df[col].ewm(span = long_win).mean(), df[col].ewm(span = short_win).mean()\n",
    "\n",
    "def RSI_feature(df, col, rsi_win = 19):\n",
    "    diff = df[col].diff(1).dropna()\n",
    "    up_chg = 0 * diff\n",
    "    down_chg = 0 * diff\n",
    "    up_chg[diff > 0]  = diff[ diff>0 ]\n",
    "    down_chg[diff < 0] =  diff[ diff < 0 ]\n",
    "    up_chg_avg   = up_chg.ewm(com=rsi_win-1 , min_periods=rsi_win).mean()\n",
    "    down_chg_avg = down_chg.ewm(com=rsi_win-1 , min_periods=rsi_win).mean()\n",
    "    rs = abs(up_chg_avg/down_chg_avg)\n",
    "    rsi = 100 - 100/(1+rs)\n",
    "    return rsi\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\frame.py:2963: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\frame.py:2963: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\frame.py:2963: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\frame.py:2963: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\frame.py:2963: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\frame.py:2963: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                AAPL AAPL_Signal       t-6       t-5       t-4       t-3  \\\n",
      "index                                                                      \n",
      "2014-03-17  0.007059        Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-18  0.010805        Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-19  0.010693        Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-20  0.008635        Hold       NaN       NaN       NaN  0.007059   \n",
      "2014-03-21  0.011987        Hold       NaN       NaN  0.007059  0.010805   \n",
      "...              ...         ...       ...       ...       ...       ...   \n",
      "2019-02-25  0.655263        Hold  0.634166  0.631828  0.634965  0.641731   \n",
      "2019-02-26  0.655879        Hold  0.631828  0.634965  0.641731  0.635765   \n",
      "2019-02-27  0.659200        Hold  0.634965  0.641731  0.635765  0.647513   \n",
      "2019-02-28  0.648620        Hold  0.641731  0.635765  0.647513  0.655263   \n",
      "2019-03-01  0.659815        Hold  0.635765  0.647513  0.655263  0.655879   \n",
      "\n",
      "                 t-2       t-1  \n",
      "index                           \n",
      "2014-03-17       NaN       NaN  \n",
      "2014-03-18       NaN  0.007059  \n",
      "2014-03-19  0.007059  0.010805  \n",
      "2014-03-20  0.010805  0.010693  \n",
      "2014-03-21  0.010693  0.008635  \n",
      "...              ...       ...  \n",
      "2019-02-25  0.635765  0.647513  \n",
      "2019-02-26  0.647513  0.655263  \n",
      "2019-02-27  0.655263  0.655879  \n",
      "2019-02-28  0.655879  0.659200  \n",
      "2019-03-01  0.659200  0.648620  \n",
      "\n",
      "[1249 rows x 8 columns]\n",
      "                MSFT MSFT_Signal       t-6       t-5       t-4       t-3  \\\n",
      "index                                                                      \n",
      "2014-03-17  0.000000        Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-18  0.016520        Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-19  0.013436        Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-20  0.025111        Hold       NaN       NaN       NaN  0.000000   \n",
      "2014-03-21  0.023238        Hold       NaN       NaN  0.000000  0.016520   \n",
      "...              ...         ...       ...       ...       ...       ...   \n",
      "2019-02-25  0.962448        Hold  0.898757  0.915029  0.914413  0.907480   \n",
      "2019-02-26  0.971981        Hold  0.915029  0.914413  0.907480  0.935460   \n",
      "2019-02-27  0.969629        Hold  0.914413  0.907480  0.935460  0.954773   \n",
      "2019-02-28  0.967896        Hold  0.907480  0.935460  0.954773  0.962448   \n",
      "2019-03-01  0.974086        Sell  0.935460  0.954773  0.962448  0.971981   \n",
      "\n",
      "                 t-2       t-1  \n",
      "index                           \n",
      "2014-03-17       NaN       NaN  \n",
      "2014-03-18       NaN  0.000000  \n",
      "2014-03-19  0.000000  0.016520  \n",
      "2014-03-20  0.016520  0.013436  \n",
      "2014-03-21  0.013436  0.025111  \n",
      "...              ...       ...  \n",
      "2019-02-25  0.935460  0.954773  \n",
      "2019-02-26  0.954773  0.962448  \n",
      "2019-02-27  0.962448  0.971981  \n",
      "2019-02-28  0.971981  0.969629  \n",
      "2019-03-01  0.969629  0.967896  \n",
      "\n",
      "[1249 rows x 8 columns]\n",
      "                 WMT WMT_Signal       t-6       t-5       t-4       t-3  \\\n",
      "index                                                                     \n",
      "2014-03-17  0.257815       Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-18  0.259255       Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-19  0.253015       Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-20  0.269018       Hold       NaN       NaN       NaN  0.257815   \n",
      "2014-03-21  0.280540       Hold       NaN       NaN  0.257815  0.259255   \n",
      "...              ...        ...       ...       ...       ...       ...   \n",
      "2019-02-25  0.857158       Hold  0.846296  0.872909  0.912918  0.870917   \n",
      "2019-02-26  0.849374       Hold  0.872909  0.912918  0.870917  0.862046   \n",
      "2019-02-27  0.838873       Hold  0.912918  0.870917  0.862046  0.864943   \n",
      "2019-02-28  0.854805       Hold  0.870917  0.862046  0.864943  0.857158   \n",
      "2019-03-01  0.835615       Hold  0.862046  0.864943  0.857158  0.849374   \n",
      "\n",
      "                 t-2       t-1  \n",
      "index                           \n",
      "2014-03-17       NaN       NaN  \n",
      "2014-03-18       NaN  0.257815  \n",
      "2014-03-19  0.257815  0.259255  \n",
      "2014-03-20  0.259255  0.253015  \n",
      "2014-03-21  0.253015  0.269018  \n",
      "...              ...       ...  \n",
      "2019-02-25  0.862046  0.864943  \n",
      "2019-02-26  0.864943  0.857158  \n",
      "2019-02-27  0.857158  0.849374  \n",
      "2019-02-28  0.849374  0.838873  \n",
      "2019-03-01  0.838873  0.854805  \n",
      "\n",
      "[1249 rows x 8 columns]\n",
      "                 IBM IBM_Signal       t-6       t-5       t-4       t-3  \\\n",
      "index                                                                     \n",
      "2014-03-17  0.768666       Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-18  0.781562       Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-19  0.754481       Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-20  0.795619       Hold       NaN       NaN       NaN  0.768666   \n",
      "2014-03-21  0.779757       Hold       NaN       NaN  0.768666  0.781562   \n",
      "...              ...        ...       ...       ...       ...       ...   \n",
      "2019-02-25  0.567268       Hold  0.520369  0.544763  0.555307  0.544291   \n",
      "2019-02-26  0.571360       Sell  0.544763  0.555307  0.544291  0.541773   \n",
      "2019-02-27  0.562704       Hold  0.555307  0.544291  0.541773  0.563964   \n",
      "2019-02-28  0.546337       Hold  0.544291  0.541773  0.563964  0.567268   \n",
      "2019-03-01  0.563176       Hold  0.541773  0.563964  0.567268  0.571360   \n",
      "\n",
      "                 t-2       t-1  \n",
      "index                           \n",
      "2014-03-17       NaN       NaN  \n",
      "2014-03-18       NaN  0.768666  \n",
      "2014-03-19  0.768666  0.781562  \n",
      "2014-03-20  0.781562  0.754481  \n",
      "2014-03-21  0.754481  0.795619  \n",
      "...              ...       ...  \n",
      "2019-02-25  0.541773  0.563964  \n",
      "2019-02-26  0.563964  0.567268  \n",
      "2019-02-27  0.567268  0.571360  \n",
      "2019-02-28  0.571360  0.562704  \n",
      "2019-03-01  0.562704  0.546337  \n",
      "\n",
      "[1249 rows x 8 columns]\n",
      "                  MU MU_Signal       t-6       t-5       t-4       t-3  \\\n",
      "index                                                                    \n",
      "2014-03-17  0.270825      Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-18  0.281380      Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-19  0.271768      Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-20  0.272333      Hold       NaN       NaN       NaN  0.270825   \n",
      "2014-03-21  0.265737      Hold       NaN       NaN  0.270825  0.281380   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "2019-02-25  0.625518      Hold  0.615718  0.611195  0.610629  0.615341   \n",
      "2019-02-26  0.629476      Sell  0.611195  0.610629  0.615341  0.602525   \n",
      "2019-02-27  0.598568      Hold  0.610629  0.615341  0.602525  0.622126   \n",
      "2019-02-28  0.590275      Hold  0.615341  0.602525  0.622126  0.625518   \n",
      "2019-03-01  0.603468      Hold  0.602525  0.622126  0.625518  0.629476   \n",
      "\n",
      "                 t-2       t-1  \n",
      "index                           \n",
      "2014-03-17       NaN       NaN  \n",
      "2014-03-18       NaN  0.270825  \n",
      "2014-03-19  0.270825  0.281380  \n",
      "2014-03-20  0.281380  0.271768  \n",
      "2014-03-21  0.271768  0.272333  \n",
      "...              ...       ...  \n",
      "2019-02-25  0.602525  0.622126  \n",
      "2019-02-26  0.622126  0.625518  \n",
      "2019-02-27  0.625518  0.629476  \n",
      "2019-02-28  0.629476  0.598568  \n",
      "2019-03-01  0.598568  0.590275  \n",
      "\n",
      "[1249 rows x 8 columns]\n",
      "                  BA BA_Signal       t-6       t-5       t-4       t-3  \\\n",
      "index                                                                    \n",
      "2014-03-17  0.028750      Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-18  0.025215      Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-19  0.020603      Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-20  0.024420      Hold       NaN       NaN       NaN  0.029959   \n",
      "2014-03-21  0.021474      Hold       NaN       NaN  0.029959  0.026274   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "2019-02-25  0.959664      Hold  0.963550  0.981253  0.967567  0.983738   \n",
      "2019-02-26  0.959370      Hold  0.988876  0.975980  0.983738  0.971602   \n",
      "2019-02-27  0.984804      Hold  0.983562  0.992291  0.971602  0.991380   \n",
      "2019-02-28  0.998064      Hold  1.000000  0.980050  0.991380  1.000000   \n",
      "2019-03-01  1.000000      Sell  0.987664  1.000000  1.000000  0.999694   \n",
      "\n",
      "                 t-2       t-1  \n",
      "index                           \n",
      "2014-03-17       NaN       NaN  \n",
      "2014-03-18       NaN  0.028806  \n",
      "2014-03-19  0.029194  0.025264  \n",
      "2014-03-20  0.025604  0.020643  \n",
      "2014-03-21  0.020921  0.024468  \n",
      "...              ...       ...  \n",
      "2019-02-25  0.946798  0.953237  \n",
      "2019-02-26  0.966071  0.961525  \n",
      "2019-02-27  0.974472  0.961231  \n",
      "2019-02-28  0.974174  0.986715  \n",
      "2019-03-01  1.000000  1.000000  \n",
      "\n",
      "[1249 rows x 8 columns]\n",
      "                 AXP AXP_Signal       t-6       t-5       t-4       t-3  \\\n",
      "index                                                                     \n",
      "2014-03-17  0.563659       Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-18  0.568455       Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-19  0.555955       Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-20  0.569908       Sell       NaN       NaN       NaN  0.563659   \n",
      "2014-03-21  0.567437       Hold       NaN       NaN  0.563659  0.568455   \n",
      "...              ...        ...       ...       ...       ...       ...   \n",
      "2019-02-25  0.913772       Hold  0.908912  0.920827  0.907030  0.911420   \n",
      "2019-02-26  0.913929       Hold  0.920827  0.907030  0.911420  0.908755   \n",
      "2019-02-27  0.921454       Hold  0.907030  0.911420  0.908755  0.921768   \n",
      "2019-02-28  0.926471       Hold  0.911420  0.908755  0.921768  0.913772   \n",
      "2019-03-01  0.944658       Hold  0.908755  0.921768  0.913772  0.913929   \n",
      "\n",
      "                 t-2       t-1  \n",
      "index                           \n",
      "2014-03-17       NaN       NaN  \n",
      "2014-03-18       NaN  0.563659  \n",
      "2014-03-19  0.563659  0.568455  \n",
      "2014-03-20  0.568455  0.555955  \n",
      "2014-03-21  0.555955  0.569908  \n",
      "...              ...       ...  \n",
      "2019-02-25  0.908755  0.921768  \n",
      "2019-02-26  0.921768  0.913772  \n",
      "2019-02-27  0.913772  0.913929  \n",
      "2019-02-28  0.913929  0.921454  \n",
      "2019-03-01  0.921454  0.926471  \n",
      "\n",
      "[1249 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\frame.py:2963: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\frame.py:2963: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\frame.py:2963: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\frame.py:2963: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\frame.py:2963: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\frame.py:2963: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                NVDA NVDA_Signal       t-6       t-5       t-4       t-3  \\\n",
      "index                                                                      \n",
      "2014-03-17  0.003073        Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-18  0.004543        Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-19  0.005663        Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-20  0.005698        Sell       NaN       NaN       NaN  0.003073   \n",
      "2014-03-21  0.005593        Hold       NaN       NaN  0.003073  0.004543   \n",
      "...              ...         ...       ...       ...       ...       ...   \n",
      "2019-02-25  0.522221        Hold  0.506977  0.517274  0.514709  0.521708   \n",
      "2019-02-26  0.516394        Hold  0.517274  0.514709  0.521708  0.511521   \n",
      "2019-02-27  0.510201        Hold  0.514709  0.521708  0.511521  0.524053   \n",
      "2019-02-28  0.506570        Hold  0.521708  0.511521  0.524053  0.522221   \n",
      "2019-03-01  0.514603        Hold  0.511521  0.524053  0.522221  0.516394   \n",
      "\n",
      "                 t-2       t-1  \n",
      "index                           \n",
      "2014-03-17       NaN       NaN  \n",
      "2014-03-18       NaN  0.003073  \n",
      "2014-03-19  0.003073  0.004543  \n",
      "2014-03-20  0.004543  0.005663  \n",
      "2014-03-21  0.005663  0.005698  \n",
      "...              ...       ...  \n",
      "2019-02-25  0.511521  0.524053  \n",
      "2019-02-26  0.524053  0.522221  \n",
      "2019-02-27  0.522221  0.516394  \n",
      "2019-02-28  0.516394  0.510201  \n",
      "2019-03-01  0.510201  0.506570  \n",
      "\n",
      "[1249 rows x 8 columns]\n",
      "                  FB FB_Signal       t-6       t-5       t-4       t-3  \\\n",
      "index                                                                    \n",
      "2014-03-17  0.078086      Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-18  0.080875      Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-19  0.074988      Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-20  0.067117      Hold       NaN       NaN       NaN  0.078086   \n",
      "2014-03-21  0.068790      Hold       NaN       NaN  0.078086  0.080875   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "2019-02-25  0.672286      Hold  0.668133  0.659147  0.657846  0.659519   \n",
      "2019-02-26  0.669249      Hold  0.659147  0.657846  0.659519  0.643902   \n",
      "2019-02-27  0.661068      Hold  0.657846  0.659519  0.643902  0.655367   \n",
      "2019-02-28  0.652640      Hold  0.659519  0.643902  0.655367  0.672286   \n",
      "2019-03-01  0.657784      Hold  0.643902  0.655367  0.672286  0.669249   \n",
      "\n",
      "                 t-2       t-1  \n",
      "index                           \n",
      "2014-03-17       NaN       NaN  \n",
      "2014-03-18       NaN  0.078086  \n",
      "2014-03-19  0.078086  0.080875  \n",
      "2014-03-20  0.080875  0.074988  \n",
      "2014-03-21  0.074988  0.067117  \n",
      "...              ...       ...  \n",
      "2019-02-25  0.643902  0.655367  \n",
      "2019-02-26  0.655367  0.672286  \n",
      "2019-02-27  0.672286  0.669249  \n",
      "2019-02-28  0.669249  0.661068  \n",
      "2019-03-01  0.661068  0.652640  \n",
      "\n",
      "[1249 rows x 8 columns]\n",
      "                 OXY OXY_Signal       t-6       t-5       t-4       t-3  \\\n",
      "index                                                                     \n",
      "2014-03-17  0.751669       Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-18  0.761729       Sell       NaN       NaN       NaN       NaN   \n",
      "2014-03-19  0.709716       Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-20  0.655495        Buy       NaN       NaN       NaN  0.751669   \n",
      "2014-03-21  0.683464       Hold       NaN       NaN  0.751669  0.761729   \n",
      "...              ...        ...       ...       ...       ...       ...   \n",
      "2019-02-25  0.437469       Hold  0.448054  0.467357  0.481056  0.500358   \n",
      "2019-02-26  0.431554       Hold  0.467357  0.481056  0.500358  0.448988   \n",
      "2019-02-27  0.450545       Hold  0.481056  0.500358  0.448988  0.428129   \n",
      "2019-02-28  0.434978       Hold  0.500358  0.448988  0.428129  0.437469   \n",
      "2019-03-01  0.478565       Sell  0.448988  0.428129  0.437469  0.431554   \n",
      "\n",
      "                 t-2       t-1  \n",
      "index                           \n",
      "2014-03-17       NaN       NaN  \n",
      "2014-03-18       NaN  0.751669  \n",
      "2014-03-19  0.751669  0.761729  \n",
      "2014-03-20  0.761729  0.709716  \n",
      "2014-03-21  0.709716  0.655495  \n",
      "...              ...       ...  \n",
      "2019-02-25  0.448988  0.428129  \n",
      "2019-02-26  0.428129  0.437469  \n",
      "2019-02-27  0.437469  0.431554  \n",
      "2019-02-28  0.431554  0.450545  \n",
      "2019-03-01  0.450545  0.434978  \n",
      "\n",
      "[1249 rows x 8 columns]\n",
      "                TSLA TSLA_Signal       t-6       t-5       t-4       t-3  \\\n",
      "index                                                                      \n",
      "2014-03-17  0.374218        Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-18  0.399329        Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-19  0.381925        Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-20  0.378072        Hold       NaN       NaN       NaN  0.374218   \n",
      "2014-03-21  0.353126        Hold       NaN       NaN  0.374218  0.399329   \n",
      "...              ...         ...       ...       ...       ...       ...   \n",
      "2019-02-25  0.642688        Hold  0.663407  0.680438  0.671156  0.658393   \n",
      "2019-02-26  0.638918        Hold  0.680438  0.671156  0.658393  0.611445   \n",
      "2019-02-27  0.708863        Hold  0.671156  0.658393  0.611445  0.625865   \n",
      "2019-02-28  0.730162        Sell  0.658393  0.611445  0.625865  0.642688   \n",
      "2019-03-01  0.626197        Hold  0.611445  0.625865  0.642688  0.638918   \n",
      "\n",
      "                 t-2       t-1  \n",
      "index                           \n",
      "2014-03-17       NaN       NaN  \n",
      "2014-03-18       NaN  0.374218  \n",
      "2014-03-19  0.374218  0.399329  \n",
      "2014-03-20  0.399329  0.381925  \n",
      "2014-03-21  0.381925  0.378072  \n",
      "...              ...       ...  \n",
      "2019-02-25  0.611445  0.625865  \n",
      "2019-02-26  0.625865  0.642688  \n",
      "2019-02-27  0.642688  0.638918  \n",
      "2019-02-28  0.638918  0.708863  \n",
      "2019-03-01  0.708863  0.730162  \n",
      "\n",
      "[1249 rows x 8 columns]\n",
      "                 JNJ JNJ_Signal       t-6       t-5       t-4       t-3  \\\n",
      "index                                                                     \n",
      "2014-03-17  0.004550       Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-18  0.004685       Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-19  0.000000       Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-20  0.007093       Hold       NaN       NaN       NaN  0.004550   \n",
      "2014-03-21  0.031317       Hold       NaN       NaN  0.004550  0.004685   \n",
      "...              ...        ...       ...       ...       ...       ...   \n",
      "2019-02-25  0.836411       Hold  0.793850  0.825390  0.814877  0.824933   \n",
      "2019-02-26  0.835030       Hold  0.825390  0.814877  0.824933  0.810763   \n",
      "2019-02-27  0.831349       Hold  0.814877  0.824933  0.810763  0.828742   \n",
      "2019-02-28  0.843159       Hold  0.824933  0.810763  0.828742  0.836411   \n",
      "2019-03-01  0.869387       Hold  0.810763  0.828742  0.836411  0.835030   \n",
      "\n",
      "                 t-2       t-1  \n",
      "index                           \n",
      "2014-03-17       NaN       NaN  \n",
      "2014-03-18       NaN  0.004550  \n",
      "2014-03-19  0.004550  0.004685  \n",
      "2014-03-20  0.004685  0.000000  \n",
      "2014-03-21  0.000000  0.007093  \n",
      "...              ...       ...  \n",
      "2019-02-25  0.810763  0.828742  \n",
      "2019-02-26  0.828742  0.836411  \n",
      "2019-02-27  0.836411  0.835030  \n",
      "2019-02-28  0.835030  0.831349  \n",
      "2019-03-01  0.831349  0.843159  \n",
      "\n",
      "[1249 rows x 8 columns]\n",
      "                NFLX NFLX_Signal       t-6       t-5       t-4       t-3  \\\n",
      "index                                                                      \n",
      "2014-03-17  0.041438        Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-18  0.040495        Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-19  0.040434        Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-20  0.042030        Hold       NaN       NaN       NaN  0.041438   \n",
      "2014-03-21  0.035050        Hold       NaN       NaN  0.041438  0.040495   \n",
      "...              ...         ...       ...       ...       ...       ...   \n",
      "2019-02-25  0.852813        Hold  0.839875  0.833994  0.847494  0.842121   \n",
      "2019-02-26  0.855647        Sell  0.833994  0.847494  0.842121  0.834261   \n",
      "2019-02-27  0.850033        Hold  0.847494  0.842121  0.834261  0.850434   \n",
      "2019-02-28  0.837282        Hold  0.842121  0.834261  0.850434  0.852813   \n",
      "2019-03-01  0.835197        Hold  0.834261  0.850434  0.852813  0.855647   \n",
      "\n",
      "                 t-2       t-1  \n",
      "index                           \n",
      "2014-03-17       NaN       NaN  \n",
      "2014-03-18       NaN  0.041438  \n",
      "2014-03-19  0.041438  0.040495  \n",
      "2014-03-20  0.040495  0.040434  \n",
      "2014-03-21  0.040434  0.042030  \n",
      "...              ...       ...  \n",
      "2019-02-25  0.834261  0.850434  \n",
      "2019-02-26  0.850434  0.852813  \n",
      "2019-02-27  0.852813  0.855647  \n",
      "2019-02-28  0.855647  0.850033  \n",
      "2019-03-01  0.850033  0.837282  \n",
      "\n",
      "[1249 rows x 8 columns]\n",
      "                 DIS DIS_Signal       t-6       t-5       t-4       t-3  \\\n",
      "index                                                                     \n",
      "2014-03-17  0.088231       Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-18  0.100318       Sell       NaN       NaN       NaN       NaN   \n",
      "2014-03-19  0.070706       Hold       NaN       NaN       NaN       NaN   \n",
      "2014-03-20  0.076547       Hold       NaN       NaN       NaN  0.088231   \n",
      "2014-03-21  0.067282       Hold       NaN       NaN  0.088231  0.100318   \n",
      "...              ...        ...       ...       ...       ...       ...   \n",
      "2019-02-25  0.904967       Hold  0.841609  0.883343  0.903237  0.906913   \n",
      "2019-02-26  0.903021       Hold  0.883343  0.903237  0.906913  0.920103   \n",
      "2019-02-27  0.887451        Buy  0.903237  0.906913  0.920103  0.940862   \n",
      "2019-02-28  0.888749       Hold  0.906913  0.920103  0.940862  0.904967   \n",
      "2019-03-01  0.914049       Hold  0.920103  0.940862  0.904967  0.903021   \n",
      "\n",
      "                 t-2       t-1  \n",
      "index                           \n",
      "2014-03-17       NaN       NaN  \n",
      "2014-03-18       NaN  0.088231  \n",
      "2014-03-19  0.088231  0.100318  \n",
      "2014-03-20  0.100318  0.070706  \n",
      "2014-03-21  0.070706  0.076547  \n",
      "...              ...       ...  \n",
      "2019-02-25  0.920103  0.940862  \n",
      "2019-02-26  0.940862  0.904967  \n",
      "2019-02-27  0.904967  0.903021  \n",
      "2019-02-28  0.903021  0.887451  \n",
      "2019-03-01  0.887451  0.888749  \n",
      "\n",
      "[1249 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\frame.py:2963: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\frame.py:2963: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\andre\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "list_of_dfs = []\n",
    "dict_of_datasets = {}\n",
    "for ticker in tickers_list:\n",
    "    # \n",
    "    df = training_df[[ticker, ticker + '_Signal']]\n",
    "    \n",
    "    df['t-6'] = df[ticker].shift(6)\n",
    "    df['t-5'] = df[ticker].shift(5)\n",
    "    df['t-4'] = df[ticker].shift(4)\n",
    "    df['t-3'] = df[ticker].shift(3)\n",
    "    df['t-2'] = df[ticker].shift(2)\n",
    "    df['t-1'] = df[ticker].shift(1)\n",
    "    \n",
    "    df[df.columns.difference([ticker + '_Signal'])] = df[df.columns.difference([ticker + '_Signal'])].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "    print(df)\n",
    "    df[ticker + '_RSI'] = RSI_feature(df, ticker)\n",
    "    df[ticker + '_slow'], df[ticker + '_fast'] = EMA_features(df, ticker)\n",
    "\n",
    "    df = df[19:]\n",
    "    df[ticker + '_Signal'] = np.where(df[ticker + '_Signal']=='Buy', 2, np.where(df[ticker + '_Signal']=='Hold', 1, 0))\n",
    "    X = pd.DataFrame(df.values, columns=df.columns)\n",
    "    y = df[ticker + '_Signal']\n",
    "\n",
    "    smote = SMOTE(random_state=45)\n",
    "    X_resampled, y_resampled = smote.fit_sample(X, y)\n",
    "\n",
    "    df = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "    train, test = train_test_split(df, test_size=0.25)\n",
    "    train, val = train_test_split(train, test_size=0.2)\n",
    "\n",
    "    \n",
    "    dict_of_datasets[ticker] = [train, val, test ]\n",
    "    \n",
    "    list_of_dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'AAPL_Signal'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2645\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2646\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'AAPL_Signal'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-810a2401942c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m19\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mticker\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_Signal'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mticker\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_Signal'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'Buy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mticker\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_Signal'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'Hold'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mticker\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_Signal'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2798\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2799\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2800\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2801\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2646\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2648\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2650\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'AAPL_Signal'"
     ]
    }
   ],
   "source": [
    "list_of_dfs = []\n",
    "dict_of_datasets = {}\n",
    "for ticker in tickers_list:\n",
    "    # \n",
    "    df = training_df[[ticker, ticker + '_Signal']]\n",
    "    df[ticker + '_RSI'] = RSI_feature(df, ticker)\n",
    "    df[ticker + '_slow'], df[ticker + '_fast'] = EMA_features(df, ticker)\n",
    "    df[['t-6', 't-5', 't-4', 't-3', 't-2', 't-1']z =  df[ticker].shift(5),  df[ticker].shift(4),\\\n",
    "                                                df[ticker].shift(3), df[ticker].shift(2), df[ticker].shift(1)   \n",
    "    print(df)\n",
    "    df = df[19:]\n",
    "    df[ticker + '_Signal'] = np.where(df[ticker + '_Signal']=='Buy', 2, np.where(df[ticker + '_Signal']=='Hold', 1, 0))\n",
    "    X = pd.DataFrame(df.values, columns=df.columns)\n",
    "    y = df[ticker + '_Signal']\n",
    "\n",
    "    smote = SMOTE(random_state=45)\n",
    "    X_resampled, y_resampled = smote.fit_sample(X, y)\n",
    "\n",
    "    df = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "    train, test = train_test_split(df, test_size=0.25)\n",
    "    train, val = train_test_split(train, test_size=0.2)\n",
    "    dict_of_datasets[ticker] = [train, val, test ]\n",
    "    \n",
    "    list_of_dfs.append(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WMT</th>\n",
       "      <th>WMT_Signal</th>\n",
       "      <th>t-5</th>\n",
       "      <th>t-4</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-2</th>\n",
       "      <th>t-1</th>\n",
       "      <th>WMT_RSI</th>\n",
       "      <th>WMT_slow</th>\n",
       "      <th>WMT_fast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.286941</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.299904</td>\n",
       "      <td>0.299904</td>\n",
       "      <td>0.313827</td>\n",
       "      <td>0.310466</td>\n",
       "      <td>0.293182</td>\n",
       "      <td>54.923452</td>\n",
       "      <td>0.295292</td>\n",
       "      <td>0.296503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.301024</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.299904</td>\n",
       "      <td>0.313827</td>\n",
       "      <td>0.310466</td>\n",
       "      <td>0.293182</td>\n",
       "      <td>0.286941</td>\n",
       "      <td>61.786133</td>\n",
       "      <td>0.296201</td>\n",
       "      <td>0.298010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.293023</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.313827</td>\n",
       "      <td>0.310466</td>\n",
       "      <td>0.293182</td>\n",
       "      <td>0.286941</td>\n",
       "      <td>0.301024</td>\n",
       "      <td>56.616479</td>\n",
       "      <td>0.295699</td>\n",
       "      <td>0.296347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.298464</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.310466</td>\n",
       "      <td>0.293182</td>\n",
       "      <td>0.286941</td>\n",
       "      <td>0.301024</td>\n",
       "      <td>0.293023</td>\n",
       "      <td>59.074285</td>\n",
       "      <td>0.296134</td>\n",
       "      <td>0.297053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.305505</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.293182</td>\n",
       "      <td>0.286941</td>\n",
       "      <td>0.301024</td>\n",
       "      <td>0.293023</td>\n",
       "      <td>0.298464</td>\n",
       "      <td>62.013988</td>\n",
       "      <td>0.297602</td>\n",
       "      <td>0.299870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.304545</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.286941</td>\n",
       "      <td>0.301024</td>\n",
       "      <td>0.293023</td>\n",
       "      <td>0.298464</td>\n",
       "      <td>0.305505</td>\n",
       "      <td>61.379383</td>\n",
       "      <td>0.298687</td>\n",
       "      <td>0.301429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.303905</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.301024</td>\n",
       "      <td>0.293023</td>\n",
       "      <td>0.298464</td>\n",
       "      <td>0.305505</td>\n",
       "      <td>0.304545</td>\n",
       "      <td>60.940507</td>\n",
       "      <td>0.299500</td>\n",
       "      <td>0.302254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.311586</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.293023</td>\n",
       "      <td>0.298464</td>\n",
       "      <td>0.305505</td>\n",
       "      <td>0.304545</td>\n",
       "      <td>0.303905</td>\n",
       "      <td>64.184094</td>\n",
       "      <td>0.301380</td>\n",
       "      <td>0.305365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.315907</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.298464</td>\n",
       "      <td>0.305505</td>\n",
       "      <td>0.304545</td>\n",
       "      <td>0.303905</td>\n",
       "      <td>0.311586</td>\n",
       "      <td>65.867063</td>\n",
       "      <td>0.303636</td>\n",
       "      <td>0.308879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.320868</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.305505</td>\n",
       "      <td>0.304545</td>\n",
       "      <td>0.303905</td>\n",
       "      <td>0.311586</td>\n",
       "      <td>0.315907</td>\n",
       "      <td>67.706104</td>\n",
       "      <td>0.306308</td>\n",
       "      <td>0.312875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.339112</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.304545</td>\n",
       "      <td>0.303905</td>\n",
       "      <td>0.311586</td>\n",
       "      <td>0.315907</td>\n",
       "      <td>0.320868</td>\n",
       "      <td>73.292024</td>\n",
       "      <td>0.311389</td>\n",
       "      <td>0.321621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.337671</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.303905</td>\n",
       "      <td>0.311586</td>\n",
       "      <td>0.315907</td>\n",
       "      <td>0.320868</td>\n",
       "      <td>0.339112</td>\n",
       "      <td>72.250490</td>\n",
       "      <td>0.315455</td>\n",
       "      <td>0.326971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.338311</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.311586</td>\n",
       "      <td>0.315907</td>\n",
       "      <td>0.320868</td>\n",
       "      <td>0.339112</td>\n",
       "      <td>0.337671</td>\n",
       "      <td>72.434301</td>\n",
       "      <td>0.318988</td>\n",
       "      <td>0.330751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.338151</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.315907</td>\n",
       "      <td>0.320868</td>\n",
       "      <td>0.339112</td>\n",
       "      <td>0.337671</td>\n",
       "      <td>0.338311</td>\n",
       "      <td>72.307572</td>\n",
       "      <td>0.321948</td>\n",
       "      <td>0.333218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.328870</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.320868</td>\n",
       "      <td>0.339112</td>\n",
       "      <td>0.337671</td>\n",
       "      <td>0.338311</td>\n",
       "      <td>0.338151</td>\n",
       "      <td>65.330876</td>\n",
       "      <td>0.323017</td>\n",
       "      <td>0.331768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.320868</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.339112</td>\n",
       "      <td>0.337671</td>\n",
       "      <td>0.338311</td>\n",
       "      <td>0.338151</td>\n",
       "      <td>0.328870</td>\n",
       "      <td>60.057446</td>\n",
       "      <td>0.322685</td>\n",
       "      <td>0.328135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.311106</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.337671</td>\n",
       "      <td>0.338311</td>\n",
       "      <td>0.338151</td>\n",
       "      <td>0.328870</td>\n",
       "      <td>0.320868</td>\n",
       "      <td>54.402705</td>\n",
       "      <td>0.320900</td>\n",
       "      <td>0.322459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.318030</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.338311</td>\n",
       "      <td>0.338151</td>\n",
       "      <td>0.328870</td>\n",
       "      <td>0.320868</td>\n",
       "      <td>0.311106</td>\n",
       "      <td>57.405437</td>\n",
       "      <td>0.320457</td>\n",
       "      <td>0.320983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.329785</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.338151</td>\n",
       "      <td>0.328870</td>\n",
       "      <td>0.320868</td>\n",
       "      <td>0.311106</td>\n",
       "      <td>0.318030</td>\n",
       "      <td>61.901417</td>\n",
       "      <td>0.321895</td>\n",
       "      <td>0.323917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.337997</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.328870</td>\n",
       "      <td>0.320868</td>\n",
       "      <td>0.311106</td>\n",
       "      <td>0.318030</td>\n",
       "      <td>0.329785</td>\n",
       "      <td>64.652571</td>\n",
       "      <td>0.324376</td>\n",
       "      <td>0.328610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         WMT  WMT_Signal       t-5       t-4       t-3       t-2       t-1  \\\n",
       "0   0.286941         2.0  0.299904  0.299904  0.313827  0.310466  0.293182   \n",
       "1   0.301024         1.0  0.299904  0.313827  0.310466  0.293182  0.286941   \n",
       "2   0.293023         1.0  0.313827  0.310466  0.293182  0.286941  0.301024   \n",
       "3   0.298464         1.0  0.310466  0.293182  0.286941  0.301024  0.293023   \n",
       "4   0.305505         1.0  0.293182  0.286941  0.301024  0.293023  0.298464   \n",
       "5   0.304545         1.0  0.286941  0.301024  0.293023  0.298464  0.305505   \n",
       "6   0.303905         1.0  0.301024  0.293023  0.298464  0.305505  0.304545   \n",
       "7   0.311586         1.0  0.293023  0.298464  0.305505  0.304545  0.303905   \n",
       "8   0.315907         1.0  0.298464  0.305505  0.304545  0.303905  0.311586   \n",
       "9   0.320868         1.0  0.305505  0.304545  0.303905  0.311586  0.315907   \n",
       "10  0.339112         0.0  0.304545  0.303905  0.311586  0.315907  0.320868   \n",
       "11  0.337671         1.0  0.303905  0.311586  0.315907  0.320868  0.339112   \n",
       "12  0.338311         1.0  0.311586  0.315907  0.320868  0.339112  0.337671   \n",
       "13  0.338151         1.0  0.315907  0.320868  0.339112  0.337671  0.338311   \n",
       "14  0.328870         1.0  0.320868  0.339112  0.337671  0.338311  0.338151   \n",
       "15  0.320868         1.0  0.339112  0.337671  0.338311  0.338151  0.328870   \n",
       "16  0.311106         2.0  0.337671  0.338311  0.338151  0.328870  0.320868   \n",
       "17  0.318030         1.0  0.338311  0.338151  0.328870  0.320868  0.311106   \n",
       "18  0.329785         1.0  0.338151  0.328870  0.320868  0.311106  0.318030   \n",
       "19  0.337997         0.0  0.328870  0.320868  0.311106  0.318030  0.329785   \n",
       "\n",
       "      WMT_RSI  WMT_slow  WMT_fast  \n",
       "0   54.923452  0.295292  0.296503  \n",
       "1   61.786133  0.296201  0.298010  \n",
       "2   56.616479  0.295699  0.296347  \n",
       "3   59.074285  0.296134  0.297053  \n",
       "4   62.013988  0.297602  0.299870  \n",
       "5   61.379383  0.298687  0.301429  \n",
       "6   60.940507  0.299500  0.302254  \n",
       "7   64.184094  0.301380  0.305365  \n",
       "8   65.867063  0.303636  0.308879  \n",
       "9   67.706104  0.306308  0.312875  \n",
       "10  73.292024  0.311389  0.321621  \n",
       "11  72.250490  0.315455  0.326971  \n",
       "12  72.434301  0.318988  0.330751  \n",
       "13  72.307572  0.321948  0.333218  \n",
       "14  65.330876  0.323017  0.331768  \n",
       "15  60.057446  0.322685  0.328135  \n",
       "16  54.402705  0.320900  0.322459  \n",
       "17  57.405437  0.320457  0.320983  \n",
       "18  61.901417  0.321895  0.323917  \n",
       "19  64.652571  0.324376  0.328610  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_dfs[2].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, ticker, shuffle=True, batch_size=32):\n",
    "    \n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop(ticker + '_Signal')\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  return ds\n",
    "\n",
    "# train_ds = df_to_dataset(list_of_dfs[0], 'AAPL', batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ({MSFT: (None,), MSFT_RSI: (None,), MSFT_slow: (None,), MSFT_fast: (None,)}, (None,)), types: ({MSFT: tf.float64, MSFT_RSI: tf.float64, MSFT_slow: tf.float64, MSFT_fast: tf.float64}, tf.int32)>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = tf.data.experimental.make_csv_dataset(\n",
    "    list_of_dfs[0],\n",
    "    batch_size,\n",
    "    column_names=column_names,\n",
    "    label_name=label_name,\n",
    "    num_epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = []\n",
    "\n",
    "# numeric cols\n",
    "for header in ['MSFT_RSI', 'MSFT_slow', 'MSFT_fast']:\n",
    "    feature_columns.append(feature_column.numeric_column(header))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ({AAPL: (None,), t-6: (None,), t-5: (None,), t-4: (None,), t-3: (None,), t-2: (None,), t-1: (None,), AAPL_RSI: (None,), AAPL_slow: (None,), AAPL_fast: (None,)}, (None,)), types: ({AAPL: tf.float32, t-6: tf.float32, t-5: tf.float32, t-4: tf.float32, t-3: tf.float32, t-2: tf.float32, t-1: tf.float32, AAPL_RSI: tf.float32, AAPL_slow: tf.float32, AAPL_fast: tf.float32}, tf.float32)>\n",
      "<BatchDataset shapes: ({AAPL: (None,), t-6: (None,), t-5: (None,), t-4: (None,), t-3: (None,), t-2: (None,), t-1: (None,), AAPL_RSI: (None,), AAPL_slow: (None,), AAPL_fast: (None,)}, (None,)), types: ({AAPL: tf.float32, t-6: tf.float32, t-5: tf.float32, t-4: tf.float32, t-3: tf.float32, t-2: tf.float32, t-1: tf.float32, AAPL_RSI: tf.float32, AAPL_slow: tf.float32, AAPL_fast: tf.float32}, tf.float32)>\n",
      "WARNING:tensorflow:Layer sequential is casting an input tensor from dtype float32 to the layer's dtype of float64, which is new behavior in TensorFlow 2.  The layer has dtype float64 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float64, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float32 by default, call `tf.keras.backend.set_floatx('float32')`. To change just this layer, pass dtype='float32' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float32 to the layer's dtype of float64, which is new behavior in TensorFlow 2.  The layer has dtype float64 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float64, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float32 by default, call `tf.keras.backend.set_floatx('float32')`. To change just this layer, pass dtype='float32' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 1/20\n",
      "62/62 [==============================] - 2s 30ms/step - loss: 22.1483 - categorical_crossentropy: 22.0663 - val_loss: 0.0000e+00 - val_categorical_crossentropy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 28.7191 - categorical_crossentropy: 28.7472 - val_loss: 33.0934 - val_categorical_crossentropy: 33.4132\n",
      "Epoch 3/20\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 29.2304 - categorical_crossentropy: 29.2222 - val_loss: 33.8296 - val_categorical_crossentropy: 33.4132\n",
      "Epoch 4/20\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 29.2545 - categorical_crossentropy: 29.2059 - val_loss: 33.3695 - val_categorical_crossentropy: 33.4132\n",
      "Epoch 5/20\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 29.0821 - categorical_crossentropy: 29.0912 - val_loss: 33.6456 - val_categorical_crossentropy: 33.4132\n",
      "Epoch 6/20\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 28.9333 - categorical_crossentropy: 28.9356 - val_loss: 33.2775 - val_categorical_crossentropy: 33.4132\n",
      "Epoch 7/20\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 28.8925 - categorical_crossentropy: 28.9192 - val_loss: 33.4615 - val_categorical_crossentropy: 33.4132\n",
      "Epoch 8/20\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 28.9170 - categorical_crossentropy: 28.9274 - val_loss: 32.6333 - val_categorical_crossentropy: 33.4132\n",
      "Epoch 9/20\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 28.9436 - categorical_crossentropy: 29.0666 - val_loss: 33.4615 - val_categorical_crossentropy: 33.4132\n",
      "Epoch 10/20\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 28.8514 - categorical_crossentropy: 28.8946 - val_loss: 33.0934 - val_categorical_crossentropy: 33.4132\n",
      "Epoch 11/20\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 29.0148 - categorical_crossentropy: 28.9520 - val_loss: 33.3695 - val_categorical_crossentropy: 33.4132\n",
      "Epoch 12/20\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 28.8343 - categorical_crossentropy: 28.8373 - val_loss: 33.4615 - val_categorical_crossentropy: 33.4132\n",
      "Epoch 13/20\n",
      "62/62 [==============================] - 0s 6ms/step - loss: 28.8918 - categorical_crossentropy: 28.8783 - val_loss: 33.0014 - val_categorical_crossentropy: 33.4132\n",
      "Epoch 14/20\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 29.1650 - categorical_crossentropy: 29.1977 - val_loss: 33.4615 - val_categorical_crossentropy: 33.4132\n",
      "Epoch 15/20\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 28.7602 - categorical_crossentropy: 28.7800 - val_loss: 33.4615 - val_categorical_crossentropy: 33.4132\n",
      "Epoch 16/20\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 28.7349 - categorical_crossentropy: 28.7226 - val_loss: 33.4615 - val_categorical_crossentropy: 33.4132\n",
      "Epoch 17/20\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 28.7857 - categorical_crossentropy: 28.8455 - val_loss: 33.2775 - val_categorical_crossentropy: 33.4132\n",
      "Epoch 18/20\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 28.5374 - categorical_crossentropy: 28.5670 - val_loss: 33.8296 - val_categorical_crossentropy: 33.4132\n",
      "Epoch 19/20\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 29.1075 - categorical_crossentropy: 29.1567 - val_loss: 33.6456 - val_categorical_crossentropy: 33.4132\n",
      "Epoch 20/20\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 28.9571 - categorical_crossentropy: 28.8946 - val_loss: 33.5535 - val_categorical_crossentropy: 33.4132\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_features (DenseFeature multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  1280      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  8256      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  2080      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  99        \n",
      "=================================================================\n",
      "Total params: 15,875\n",
      "Trainable params: 15,875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 31.1901 - categorical_crossentropy: 31.1760\n",
      "\n",
      "\n",
      "\n",
      "Accuracy 31.176048688333506 31.190064411896927\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<BatchDataset shapes: ({MSFT: (None,), t-6: (None,), t-5: (None,), t-4: (None,), t-3: (None,), t-2: (None,), t-1: (None,), MSFT_RSI: (None,), MSFT_slow: (None,), MSFT_fast: (None,)}, (None,)), types: ({MSFT: tf.float32, t-6: tf.float32, t-5: tf.float32, t-4: tf.float32, t-3: tf.float32, t-2: tf.float32, t-1: tf.float32, MSFT_RSI: tf.float32, MSFT_slow: tf.float32, MSFT_fast: tf.float32}, tf.float32)>\n",
      "<BatchDataset shapes: ({MSFT: (None,), t-6: (None,), t-5: (None,), t-4: (None,), t-3: (None,), t-2: (None,), t-1: (None,), MSFT_RSI: (None,), MSFT_slow: (None,), MSFT_fast: (None,)}, (None,)), types: ({MSFT: tf.float32, t-6: tf.float32, t-5: tf.float32, t-4: tf.float32, t-3: tf.float32, t-2: tf.float32, t-1: tf.float32, MSFT_RSI: tf.float32, MSFT_slow: tf.float32, MSFT_fast: tf.float32}, tf.float32)>\n",
      "WARNING:tensorflow:Layer sequential_1 is casting an input tensor from dtype float32 to the layer's dtype of float64, which is new behavior in TensorFlow 2.  The layer has dtype float64 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float64, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float32 by default, call `tf.keras.backend.set_floatx('float32')`. To change just this layer, pass dtype='float32' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense_5 is casting an input tensor from dtype float32 to the layer's dtype of float64, which is new behavior in TensorFlow 2.  The layer has dtype float64 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float64, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float32 by default, call `tf.keras.backend.set_floatx('float32')`. To change just this layer, pass dtype='float32' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 1/20\n",
      "60/60 [==============================] - 1s 22ms/step - loss: 4.3267 - categorical_crossentropy: 4.3276 - val_loss: 0.0000e+00 - val_categorical_crossentropy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 3.3950 - categorical_crossentropy: 3.4004 - val_loss: 3.2684 - val_categorical_crossentropy: 3.2684\n",
      "Epoch 3/20\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 3.4018 - categorical_crossentropy: 3.3970 - val_loss: 3.2684 - val_categorical_crossentropy: 3.2684\n",
      "Epoch 4/20\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 3.3919 - categorical_crossentropy: 3.3957 - val_loss: 3.2685 - val_categorical_crossentropy: 3.2685\n",
      "Epoch 5/20\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 3.4100 - categorical_crossentropy: 3.3949 - val_loss: 3.2684 - val_categorical_crossentropy: 3.2684\n",
      "Epoch 6/20\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 3.3884 - categorical_crossentropy: 3.3939 - val_loss: 3.2684 - val_categorical_crossentropy: 3.2684\n",
      "Epoch 7/20\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 3.3898 - categorical_crossentropy: 3.3935 - val_loss: 3.2684 - val_categorical_crossentropy: 3.2684\n",
      "Epoch 8/20\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 3.3911 - categorical_crossentropy: 3.3932 - val_loss: 3.2684 - val_categorical_crossentropy: 3.2684\n",
      "Epoch 9/20\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 3.3979 - categorical_crossentropy: 3.3930 - val_loss: 3.2685 - val_categorical_crossentropy: 3.2685\n",
      "Epoch 10/20\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 3.4062 - categorical_crossentropy: 3.3928 - val_loss: 3.2684 - val_categorical_crossentropy: 3.2684\n",
      "Epoch 11/20\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 3.3890 - categorical_crossentropy: 3.3927 - val_loss: 3.2684 - val_categorical_crossentropy: 3.2684\n",
      "Epoch 12/20\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 3.4009 - categorical_crossentropy: 3.3926 - val_loss: 3.2684 - val_categorical_crossentropy: 3.2684\n",
      "Epoch 13/20\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 3.3939 - categorical_crossentropy: 3.3925 - val_loss: 3.2684 - val_categorical_crossentropy: 3.2684\n",
      "Epoch 14/20\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 3.3904 - categorical_crossentropy: 3.3925 - val_loss: 3.2684 - val_categorical_crossentropy: 3.2684\n",
      "Epoch 15/20\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 3.4007 - categorical_crossentropy: 3.3924 - val_loss: 3.2684 - val_categorical_crossentropy: 3.2684\n",
      "Epoch 16/20\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 3.3852 - categorical_crossentropy: 3.3924 - val_loss: 3.2684 - val_categorical_crossentropy: 3.2684\n",
      "Epoch 17/20\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 3.3852 - categorical_crossentropy: 3.3923 - val_loss: 3.2684 - val_categorical_crossentropy: 3.2684\n",
      "Epoch 18/20\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 3.3800 - categorical_crossentropy: 3.3923 - val_loss: 3.2684 - val_categorical_crossentropy: 3.2684\n",
      "Epoch 19/20\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 3.4057 - categorical_crossentropy: 3.3923 - val_loss: 3.2684 - val_categorical_crossentropy: 3.2684\n",
      "Epoch 20/20\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 3.3851 - categorical_crossentropy: 3.3923 - val_loss: 3.2684 - val_categorical_crossentropy: 3.2684\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_features_1 (DenseFeatu multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  1280      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              multiple                  8256      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              multiple                  2080      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              multiple                  99        \n",
      "=================================================================\n",
      "Total params: 15,875\n",
      "Trainable params: 15,875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 3.0824 - categorical_crossentropy: 3.0813\n",
      "\n",
      "\n",
      "\n",
      "Accuracy 3.0813432361857043 3.082408732031622\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<BatchDataset shapes: ({WMT: (None,), t-6: (None,), t-5: (None,), t-4: (None,), t-3: (None,), t-2: (None,), t-1: (None,), WMT_RSI: (None,), WMT_slow: (None,), WMT_fast: (None,)}, (None,)), types: ({WMT: tf.float32, t-6: tf.float32, t-5: tf.float32, t-4: tf.float32, t-3: tf.float32, t-2: tf.float32, t-1: tf.float32, WMT_RSI: tf.float32, WMT_slow: tf.float32, WMT_fast: tf.float32}, tf.float32)>\n",
      "<BatchDataset shapes: ({WMT: (None,), t-6: (None,), t-5: (None,), t-4: (None,), t-3: (None,), t-2: (None,), t-1: (None,), WMT_RSI: (None,), WMT_slow: (None,), WMT_fast: (None,)}, (None,)), types: ({WMT: tf.float32, t-6: tf.float32, t-5: tf.float32, t-4: tf.float32, t-3: tf.float32, t-2: tf.float32, t-1: tf.float32, WMT_RSI: tf.float32, WMT_slow: tf.float32, WMT_fast: tf.float32}, tf.float32)>\n",
      "WARNING:tensorflow:Layer sequential_2 is casting an input tensor from dtype float32 to the layer's dtype of float64, which is new behavior in TensorFlow 2.  The layer has dtype float64 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float64, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float32 by default, call `tf.keras.backend.set_floatx('float32')`. To change just this layer, pass dtype='float32' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_10 is casting an input tensor from dtype float32 to the layer's dtype of float64, which is new behavior in TensorFlow 2.  The layer has dtype float64 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float64, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float32 by default, call `tf.keras.backend.set_floatx('float32')`. To change just this layer, pass dtype='float32' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 1/20\n",
      "61/61 [==============================] - 1s 22ms/step - loss: 24.0549 - categorical_crossentropy: 24.0217 - val_loss: 0.0000e+00 - val_categorical_crossentropy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 10.5384 - categorical_crossentropy: 10.4772 - val_loss: 3.2555 - val_categorical_crossentropy: 3.2899\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 0s 5ms/step - loss: 3.2785 - categorical_crossentropy: 3.2736 - val_loss: 3.3243 - val_categorical_crossentropy: 3.2892\n",
      "Epoch 4/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 3.2802 - categorical_crossentropy: 3.2719 - val_loss: 3.2548 - val_categorical_crossentropy: 3.2892\n",
      "Epoch 5/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 3.2694 - categorical_crossentropy: 3.2714 - val_loss: 3.2546 - val_categorical_crossentropy: 3.2891\n",
      "Epoch 6/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 3.2758 - categorical_crossentropy: 3.2710 - val_loss: 3.3589 - val_categorical_crossentropy: 3.2890\n",
      "Epoch 7/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 3.2707 - categorical_crossentropy: 3.2709 - val_loss: 3.3589 - val_categorical_crossentropy: 3.2890\n",
      "Epoch 8/20\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 3.2706 - categorical_crossentropy: 3.2708 - val_loss: 3.3242 - val_categorical_crossentropy: 3.2890\n",
      "Epoch 9/20\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 3.2705 - categorical_crossentropy: 3.2708 - val_loss: 3.3589 - val_categorical_crossentropy: 3.2891\n",
      "Epoch 10/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 3.2671 - categorical_crossentropy: 3.2708 - val_loss: 3.2199 - val_categorical_crossentropy: 3.2890\n",
      "Epoch 11/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 3.2823 - categorical_crossentropy: 3.2707 - val_loss: 3.3242 - val_categorical_crossentropy: 3.2891\n",
      "Epoch 12/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 3.2738 - categorical_crossentropy: 3.2707 - val_loss: 3.2894 - val_categorical_crossentropy: 3.2890\n",
      "Epoch 13/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 3.2671 - categorical_crossentropy: 3.2707 - val_loss: 3.2894 - val_categorical_crossentropy: 3.2890\n",
      "Epoch 14/20\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 3.2636 - categorical_crossentropy: 3.2706 - val_loss: 3.2894 - val_categorical_crossentropy: 3.2890\n",
      "Epoch 15/20\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 3.2636 - categorical_crossentropy: 3.2706 - val_loss: 3.2894 - val_categorical_crossentropy: 3.2890\n",
      "Epoch 16/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 3.2720 - categorical_crossentropy: 3.2705 - val_loss: 3.3937 - val_categorical_crossentropy: 3.2891\n",
      "Epoch 17/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 3.2754 - categorical_crossentropy: 3.2705 - val_loss: 3.2199 - val_categorical_crossentropy: 3.2890\n",
      "Epoch 18/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 3.2720 - categorical_crossentropy: 3.2705 - val_loss: 3.2894 - val_categorical_crossentropy: 3.2890\n",
      "Epoch 19/20\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 3.2770 - categorical_crossentropy: 3.2705 - val_loss: 3.3242 - val_categorical_crossentropy: 3.2890\n",
      "Epoch 20/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 3.2619 - categorical_crossentropy: 3.2705 - val_loss: 3.3589 - val_categorical_crossentropy: 3.2890\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_features_2 (DenseFeatu multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             multiple                  1280      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             multiple                  8256      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             multiple                  2080      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             multiple                  99        \n",
      "=================================================================\n",
      "Total params: 15,875\n",
      "Trainable params: 15,875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 3.3473 - categorical_crossentropy: 3.3611\n",
      "\n",
      "\n",
      "\n",
      "Accuracy 3.3611026797397416 3.347336022899701\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<BatchDataset shapes: ({IBM: (None,), t-6: (None,), t-5: (None,), t-4: (None,), t-3: (None,), t-2: (None,), t-1: (None,), IBM_RSI: (None,), IBM_slow: (None,), IBM_fast: (None,)}, (None,)), types: ({IBM: tf.float64, t-6: tf.float64, t-5: tf.float64, t-4: tf.float64, t-3: tf.float64, t-2: tf.float64, t-1: tf.float64, IBM_RSI: tf.float64, IBM_slow: tf.float64, IBM_fast: tf.float64}, tf.float64)>\n",
      "<BatchDataset shapes: ({IBM: (None,), t-6: (None,), t-5: (None,), t-4: (None,), t-3: (None,), t-2: (None,), t-1: (None,), IBM_RSI: (None,), IBM_slow: (None,), IBM_fast: (None,)}, (None,)), types: ({IBM: tf.float64, t-6: tf.float64, t-5: tf.float64, t-4: tf.float64, t-3: tf.float64, t-2: tf.float64, t-1: tf.float64, IBM_RSI: tf.float64, IBM_slow: tf.float64, IBM_fast: tf.float64}, tf.float64)>\n",
      "WARNING:tensorflow:Layer dense_15 is casting an input tensor from dtype float32 to the layer's dtype of float64, which is new behavior in TensorFlow 2.  The layer has dtype float64 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float64, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float32 by default, call `tf.keras.backend.set_floatx('float32')`. To change just this layer, pass dtype='float32' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 1/20\n",
      "61/61 [==============================] - 1s 20ms/step - loss: 5.4760 - categorical_crossentropy: 5.4814 - val_loss: 0.0000e+00 - val_categorical_crossentropy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 3.3082 - categorical_crossentropy: 3.3047 - val_loss: 3.2645 - val_categorical_crossentropy: 3.2832\n",
      "Epoch 3/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 3.2981 - categorical_crossentropy: 3.2998 - val_loss: 3.2261 - val_categorical_crossentropy: 3.2834\n",
      "Epoch 4/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 3.2958 - categorical_crossentropy: 3.2959 - val_loss: 3.2062 - val_categorical_crossentropy: 3.2828\n",
      "Epoch 5/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 3.2975 - categorical_crossentropy: 3.2942 - val_loss: 3.2642 - val_categorical_crossentropy: 3.2829\n",
      "Epoch 6/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 3.2900 - categorical_crossentropy: 3.2934 - val_loss: 3.3413 - val_categorical_crossentropy: 3.2827\n",
      "Epoch 7/20\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 3.2861 - categorical_crossentropy: 3.2929 - val_loss: 3.2637 - val_categorical_crossentropy: 3.2824\n",
      "Epoch 8/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 3.3056 - categorical_crossentropy: 3.2922 - val_loss: 3.2637 - val_categorical_crossentropy: 3.2824\n",
      "Epoch 9/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 3.2902 - categorical_crossentropy: 3.2919 - val_loss: 3.3796 - val_categorical_crossentropy: 3.2824\n",
      "Epoch 10/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 3.3001 - categorical_crossentropy: 3.2918 - val_loss: 3.2830 - val_categorical_crossentropy: 3.2824\n",
      "Epoch 11/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 3.2831 - categorical_crossentropy: 3.2916 - val_loss: 3.3023 - val_categorical_crossentropy: 3.2823\n",
      "Epoch 12/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 3.3032 - categorical_crossentropy: 3.2915 - val_loss: 3.3216 - val_categorical_crossentropy: 3.2824\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 0s 5ms/step - loss: 3.2863 - categorical_crossentropy: 3.2914 - val_loss: 3.3023 - val_categorical_crossentropy: 3.2824\n",
      "Epoch 14/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 3.2980 - categorical_crossentropy: 3.2913 - val_loss: 3.3216 - val_categorical_crossentropy: 3.2823\n",
      "Epoch 15/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 3.2793 - categorical_crossentropy: 3.2912 - val_loss: 3.2637 - val_categorical_crossentropy: 3.2823\n",
      "Epoch 16/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 3.2826 - categorical_crossentropy: 3.2911 - val_loss: 3.2830 - val_categorical_crossentropy: 3.2823\n",
      "Epoch 17/20\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 3.2978 - categorical_crossentropy: 3.2911 - val_loss: 3.2830 - val_categorical_crossentropy: 3.2823\n",
      "Epoch 18/20\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 3.2945 - categorical_crossentropy: 3.2912 - val_loss: 3.3216 - val_categorical_crossentropy: 3.2824\n",
      "Epoch 19/20\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 3.2775 - categorical_crossentropy: 3.2911 - val_loss: 3.3216 - val_categorical_crossentropy: 3.2823\n",
      "Epoch 20/20\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 3.2826 - categorical_crossentropy: 3.2911 - val_loss: 3.2830 - val_categorical_crossentropy: 3.2823\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_features_3 (DenseFeatu multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             multiple                  1280      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             multiple                  8256      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             multiple                  2080      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             multiple                  99        \n",
      "=================================================================\n",
      "Total params: 15,875\n",
      "Trainable params: 15,875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 3.3446 - categorical_crossentropy: 3.3161\n",
      "\n",
      "\n",
      "\n",
      "Accuracy 3.3161124839683485 3.3445978446604583\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<BatchDataset shapes: ({MU: (None,), t-6: (None,), t-5: (None,), t-4: (None,), t-3: (None,), t-2: (None,), t-1: (None,), MU_RSI: (None,), MU_slow: (None,), MU_fast: (None,)}, (None,)), types: ({MU: tf.float32, t-6: tf.float32, t-5: tf.float32, t-4: tf.float32, t-3: tf.float32, t-2: tf.float32, t-1: tf.float32, MU_RSI: tf.float32, MU_slow: tf.float32, MU_fast: tf.float32}, tf.float32)>\n",
      "<BatchDataset shapes: ({MU: (None,), t-6: (None,), t-5: (None,), t-4: (None,), t-3: (None,), t-2: (None,), t-1: (None,), MU_RSI: (None,), MU_slow: (None,), MU_fast: (None,)}, (None,)), types: ({MU: tf.float32, t-6: tf.float32, t-5: tf.float32, t-4: tf.float32, t-3: tf.float32, t-2: tf.float32, t-1: tf.float32, MU_RSI: tf.float32, MU_slow: tf.float32, MU_fast: tf.float32}, tf.float32)>\n",
      "WARNING:tensorflow:Layer sequential_4 is casting an input tensor from dtype float32 to the layer's dtype of float64, which is new behavior in TensorFlow 2.  The layer has dtype float64 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float64, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float32 by default, call `tf.keras.backend.set_floatx('float32')`. To change just this layer, pass dtype='float32' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_20 is casting an input tensor from dtype float32 to the layer's dtype of float64, which is new behavior in TensorFlow 2.  The layer has dtype float64 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float64, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float32 by default, call `tf.keras.backend.set_floatx('float32')`. To change just this layer, pass dtype='float32' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 1/20\n",
      "61/61 [==============================] - 1s 24ms/step - loss: 22.4895 - categorical_crossentropy: 22.5161 - val_loss: 0.0000e+00 - val_categorical_crossentropy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 15.8836 - categorical_crossentropy: 15.9185 - val_loss: 15.9040 - val_categorical_crossentropy: 16.2510\n",
      "Epoch 3/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 15.9332 - categorical_crossentropy: 15.9185 - val_loss: 15.7340 - val_categorical_crossentropy: 16.2510\n",
      "Epoch 4/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 15.8919 - categorical_crossentropy: 15.9185 - val_loss: 16.5840 - val_categorical_crossentropy: 16.2510\n",
      "Epoch 5/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 15.8836 - categorical_crossentropy: 15.9185 - val_loss: 15.7340 - val_categorical_crossentropy: 16.2510\n",
      "Epoch 6/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 15.9084 - categorical_crossentropy: 15.9185 - val_loss: 16.2440 - val_categorical_crossentropy: 16.2510\n",
      "Epoch 7/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 15.9332 - categorical_crossentropy: 15.9185 - val_loss: 16.4140 - val_categorical_crossentropy: 16.2510\n",
      "Epoch 8/20\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 15.9332 - categorical_crossentropy: 15.9185 - val_loss: 16.4140 - val_categorical_crossentropy: 16.2510\n",
      "Epoch 9/20\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 15.8754 - categorical_crossentropy: 15.9185 - val_loss: 16.5840 - val_categorical_crossentropy: 16.2510\n",
      "Epoch 10/20\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 15.9003 - categorical_crossentropy: 15.9268 - val_loss: 16.0740 - val_categorical_crossentropy: 16.2510\n",
      "Epoch 11/20\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 15.9084 - categorical_crossentropy: 15.9185 - val_loss: 16.4140 - val_categorical_crossentropy: 16.2510\n",
      "Epoch 12/20\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 15.8919 - categorical_crossentropy: 15.9185 - val_loss: 16.0740 - val_categorical_crossentropy: 16.2510\n",
      "Epoch 13/20\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 15.9249 - categorical_crossentropy: 15.9185 - val_loss: 15.5640 - val_categorical_crossentropy: 16.2510\n",
      "Epoch 14/20\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 15.9332 - categorical_crossentropy: 15.9185 - val_loss: 15.9040 - val_categorical_crossentropy: 16.2510\n",
      "Epoch 15/20\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 15.9084 - categorical_crossentropy: 15.9185 - val_loss: 16.5840 - val_categorical_crossentropy: 16.2510\n",
      "Epoch 16/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 15.9498 - categorical_crossentropy: 15.9268 - val_loss: 16.5840 - val_categorical_crossentropy: 16.2510\n",
      "Epoch 17/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 15.9827 - categorical_crossentropy: 15.9185 - val_loss: 16.2440 - val_categorical_crossentropy: 16.2510\n",
      "Epoch 18/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 15.9497 - categorical_crossentropy: 15.9185 - val_loss: 15.9040 - val_categorical_crossentropy: 16.2510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 15.9332 - categorical_crossentropy: 15.9185 - val_loss: 16.2440 - val_categorical_crossentropy: 16.2510\n",
      "Epoch 20/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 15.8836 - categorical_crossentropy: 15.9185 - val_loss: 16.2440 - val_categorical_crossentropy: 16.2510\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_features_4 (DenseFeatu multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             multiple                  1280      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             multiple                  8256      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             multiple                  2080      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             multiple                  99        \n",
      "=================================================================\n",
      "Total params: 15,875\n",
      "Trainable params: 15,875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 16.3893 - categorical_crossentropy: 16.5171\n",
      "\n",
      "\n",
      "\n",
      "Accuracy 16.517058619546404 16.389313349357018\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<BatchDataset shapes: ({BA: (None,), t-6: (None,), t-5: (None,), t-4: (None,), t-3: (None,), t-2: (None,), t-1: (None,), BA_RSI: (None,), BA_slow: (None,), BA_fast: (None,)}, (None,)), types: ({BA: tf.float64, t-6: tf.float64, t-5: tf.float64, t-4: tf.float64, t-3: tf.float64, t-2: tf.float64, t-1: tf.float64, BA_RSI: tf.float64, BA_slow: tf.float64, BA_fast: tf.float64}, tf.float64)>\n",
      "<BatchDataset shapes: ({BA: (None,), t-6: (None,), t-5: (None,), t-4: (None,), t-3: (None,), t-2: (None,), t-1: (None,), BA_RSI: (None,), BA_slow: (None,), BA_fast: (None,)}, (None,)), types: ({BA: tf.float64, t-6: tf.float64, t-5: tf.float64, t-4: tf.float64, t-3: tf.float64, t-2: tf.float64, t-1: tf.float64, BA_RSI: tf.float64, BA_slow: tf.float64, BA_fast: tf.float64}, tf.float64)>\n",
      "WARNING:tensorflow:Layer dense_25 is casting an input tensor from dtype float32 to the layer's dtype of float64, which is new behavior in TensorFlow 2.  The layer has dtype float64 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float64, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float32 by default, call `tf.keras.backend.set_floatx('float32')`. To change just this layer, pass dtype='float32' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 1/20\n",
      "61/61 [==============================] - 1s 20ms/step - loss: 22.1210 - categorical_crossentropy: 22.1926 - val_loss: 0.0000e+00 - val_categorical_crossentropy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 32.5303 - categorical_crossentropy: 32.5454 - val_loss: 35.1324 - val_categorical_crossentropy: 33.3708\n",
      "Epoch 3/20\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 32.6472 - categorical_crossentropy: 32.6289 - val_loss: 34.5238 - val_categorical_crossentropy: 33.3708\n",
      "Epoch 4/20\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 32.5054 - categorical_crossentropy: 32.5370 - val_loss: 33.3065 - val_categorical_crossentropy: 33.3708\n",
      "Epoch 5/20\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 32.6386 - categorical_crossentropy: 32.6038 - val_loss: 33.3065 - val_categorical_crossentropy: 33.3708\n",
      "Epoch 6/20\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 32.5305 - categorical_crossentropy: 32.5621 - val_loss: 34.5238 - val_categorical_crossentropy: 33.3708\n",
      "Epoch 7/20\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 32.5311 - categorical_crossentropy: 32.5955 - val_loss: 31.4807 - val_categorical_crossentropy: 33.3708\n",
      "Epoch 8/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 32.6055 - categorical_crossentropy: 32.6038 - val_loss: 33.9152 - val_categorical_crossentropy: 33.3708\n",
      "Epoch 9/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 32.5720 - categorical_crossentropy: 32.5704 - val_loss: 32.0893 - val_categorical_crossentropy: 33.3708\n",
      "Epoch 10/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 32.6229 - categorical_crossentropy: 32.6540 - val_loss: 33.9152 - val_categorical_crossentropy: 33.3708\n",
      "Epoch 11/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 32.5227 - categorical_crossentropy: 32.5871 - val_loss: 35.1324 - val_categorical_crossentropy: 33.3708\n",
      "Epoch 12/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 32.6727 - categorical_crossentropy: 32.6707 - val_loss: 32.6979 - val_categorical_crossentropy: 33.3708\n",
      "Epoch 13/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 32.5977 - categorical_crossentropy: 32.6289 - val_loss: 33.9152 - val_categorical_crossentropy: 33.3708\n",
      "Epoch 14/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 32.5809 - categorical_crossentropy: 32.6122 - val_loss: 35.1324 - val_categorical_crossentropy: 33.3708\n",
      "Epoch 15/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 32.6632 - categorical_crossentropy: 32.5955 - val_loss: 32.6979 - val_categorical_crossentropy: 33.3708\n",
      "Epoch 16/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 32.5969 - categorical_crossentropy: 32.5788 - val_loss: 33.3065 - val_categorical_crossentropy: 33.3708\n",
      "Epoch 17/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 32.6126 - categorical_crossentropy: 32.5286 - val_loss: 33.9152 - val_categorical_crossentropy: 33.3708\n",
      "Epoch 18/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 32.5552 - categorical_crossentropy: 32.5537 - val_loss: 34.5238 - val_categorical_crossentropy: 33.3708\n",
      "Epoch 19/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 32.5640 - categorical_crossentropy: 32.5871 - val_loss: 34.5238 - val_categorical_crossentropy: 33.3708\n",
      "Epoch 20/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 32.5814 - categorical_crossentropy: 32.6456 - val_loss: 34.5238 - val_categorical_crossentropy: 33.3708\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_features_5 (DenseFeatu multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             multiple                  1280      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             multiple                  8256      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             multiple                  2080      \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             multiple                  99        \n",
      "=================================================================\n",
      "Total params: 15,875\n",
      "Trainable params: 15,875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 3ms/step - loss: 30.2214 - categorical_crossentropy: 30.4319\n",
      "\n",
      "\n",
      "\n",
      "Accuracy 30.431926957779517 30.221428871154785\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<BatchDataset shapes: ({AXP: (None,), t-6: (None,), t-5: (None,), t-4: (None,), t-3: (None,), t-2: (None,), t-1: (None,), AXP_RSI: (None,), AXP_slow: (None,), AXP_fast: (None,)}, (None,)), types: ({AXP: tf.float32, t-6: tf.float32, t-5: tf.float32, t-4: tf.float32, t-3: tf.float32, t-2: tf.float32, t-1: tf.float32, AXP_RSI: tf.float32, AXP_slow: tf.float32, AXP_fast: tf.float32}, tf.float32)>\n",
      "<BatchDataset shapes: ({AXP: (None,), t-6: (None,), t-5: (None,), t-4: (None,), t-3: (None,), t-2: (None,), t-1: (None,), AXP_RSI: (None,), AXP_slow: (None,), AXP_fast: (None,)}, (None,)), types: ({AXP: tf.float32, t-6: tf.float32, t-5: tf.float32, t-4: tf.float32, t-3: tf.float32, t-2: tf.float32, t-1: tf.float32, AXP_RSI: tf.float32, AXP_slow: tf.float32, AXP_fast: tf.float32}, tf.float32)>\n",
      "WARNING:tensorflow:Layer sequential_6 is casting an input tensor from dtype float32 to the layer's dtype of float64, which is new behavior in TensorFlow 2.  The layer has dtype float64 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float64, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float32 by default, call `tf.keras.backend.set_floatx('float32')`. To change just this layer, pass dtype='float32' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_30 is casting an input tensor from dtype float32 to the layer's dtype of float64, which is new behavior in TensorFlow 2.  The layer has dtype float64 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float64, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float32 by default, call `tf.keras.backend.set_floatx('float32')`. To change just this layer, pass dtype='float32' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 1/20\n",
      "61/61 [==============================] - 1s 21ms/step - loss: 16.8317 - categorical_crossentropy: 16.9042 - val_loss: 0.0000e+00 - val_categorical_crossentropy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 18.4446 - categorical_crossentropy: 18.4175 - val_loss: 16.1811 - val_categorical_crossentropy: 16.1848\n",
      "Epoch 3/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 25.7473 - categorical_crossentropy: 25.7689 - val_loss: 33.5794 - val_categorical_crossentropy: 32.3697\n",
      "Epoch 4/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 30.0083 - categorical_crossentropy: 29.9856 - val_loss: 31.1449 - val_categorical_crossentropy: 32.3697\n",
      "Epoch 5/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 18.6016 - categorical_crossentropy: 18.5491 - val_loss: 15.8767 - val_categorical_crossentropy: 16.1848\n",
      "Epoch 6/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 16.2436 - categorical_crossentropy: 16.2101 - val_loss: 16.4854 - val_categorical_crossentropy: 16.1848\n",
      "Epoch 7/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 16.2187 - categorical_crossentropy: 16.2101 - val_loss: 16.7897 - val_categorical_crossentropy: 16.1848\n",
      "Epoch 8/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 16.2187 - categorical_crossentropy: 16.2100 - val_loss: 16.4854 - val_categorical_crossentropy: 16.1848\n",
      "Epoch 9/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 16.2517 - categorical_crossentropy: 16.2100 - val_loss: 15.8767 - val_categorical_crossentropy: 16.1848\n",
      "Epoch 10/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 16.2352 - categorical_crossentropy: 16.2100 - val_loss: 16.1811 - val_categorical_crossentropy: 16.1848\n",
      "Epoch 11/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 16.2022 - categorical_crossentropy: 16.2100 - val_loss: 15.8767 - val_categorical_crossentropy: 16.1848\n",
      "Epoch 12/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 16.1939 - categorical_crossentropy: 16.2100 - val_loss: 16.4854 - val_categorical_crossentropy: 16.1848\n",
      "Epoch 13/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 16.2022 - categorical_crossentropy: 16.2100 - val_loss: 16.1811 - val_categorical_crossentropy: 16.1848\n",
      "Epoch 14/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 16.2269 - categorical_crossentropy: 16.2100 - val_loss: 16.4854 - val_categorical_crossentropy: 16.1848\n",
      "Epoch 15/20\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 16.2104 - categorical_crossentropy: 16.2100 - val_loss: 16.7897 - val_categorical_crossentropy: 16.1848\n",
      "Epoch 16/20\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 16.2187 - categorical_crossentropy: 16.2100 - val_loss: 16.7897 - val_categorical_crossentropy: 16.1848\n",
      "Epoch 17/20\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 16.2022 - categorical_crossentropy: 16.2100 - val_loss: 15.5724 - val_categorical_crossentropy: 16.1848\n",
      "Epoch 18/20\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 16.2104 - categorical_crossentropy: 16.2100 - val_loss: 15.8767 - val_categorical_crossentropy: 16.1848\n",
      "Epoch 19/20\n",
      "61/61 [==============================] - ETA: 0s - loss: 16.5099 - categorical_crossentropy: 16.50 - 0s 5ms/step - loss: 16.2105 - categorical_crossentropy: 16.2100 - val_loss: 16.4854 - val_categorical_crossentropy: 16.1848\n",
      "Epoch 20/20\n",
      " 1/61 [..............................] - ETA: 0s - loss: 15.6144 - categorical_crossentropy: 15.6144"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.set_floatx('float64')\n",
    "dict_of_models = {}\n",
    "for ticker in tickers_list:\n",
    "    feature_columns = [feature_column.numeric_column(x) for x in ['t-6','t-5', 't-4', 't-3', 't-2','t-1',\\\n",
    "                                                                     ticker + '_RSI', ticker +'_slow', ticker +'_fast']]\n",
    "    feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "    \n",
    "    train_ds = df_to_dataset(dict_of_datasets[ticker][0], ticker)\n",
    "    print(train_ds)\n",
    "    val_ds = df_to_dataset(dict_of_datasets[ticker][1], ticker)\n",
    "    test_ds = df_to_dataset(dict_of_datasets[ticker][2], ticker)\n",
    "    \n",
    "    print(train_ds)\n",
    "    model = tf.keras.Sequential([\n",
    "      feature_layer,\n",
    "      layers.Dense(128, activation='relu'), \n",
    "      layers.Dense(64, activation='relu'),\n",
    "      layers.Dropout(.1),\n",
    "      layers.Dense(64, activation='relu'),\n",
    "      layers.Dense(32, activation='relu'),\n",
    "        \n",
    "      layers.Dense(3)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss= ['categorical_crossentropy'],\n",
    "                  metrics= ['categorical_crossentropy']\n",
    ")\n",
    "    \n",
    "    \n",
    "    model.fit(train_ds,\n",
    "              validation_data=val_ds,\n",
    "              epochs=20)\n",
    "    \n",
    "    model.summary()\n",
    "    loss, accuracy = model.evaluate(test_ds)\n",
    "    print(\"\\n\\n\\nAccuracy\", accuracy, loss)\n",
    "    print('\\n\\n\\n\\n')\n",
    "\n",
    "    \n",
    "    dict_of_models[ticker] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = dict_of_models['AAPL'].predict(df_to_dataset(dict_of_datasets['AAPL'][2], 'AAPL'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 13.96163029,   2.28275384,  -9.05586184],\n",
       "       [ 17.21345797,  10.30027803, -13.36064236],\n",
       "       [ 25.35486072,  13.5512408 , -19.66311497],\n",
       "       ...,\n",
       "       [ 29.30464284,  19.06655923, -22.18027421],\n",
       "       [ 28.50264198,  16.86085038, -21.98085977],\n",
       "       [ 17.17712292,   7.06722457, -13.17746353]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 2\n",
      " 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
      " 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0\n",
      " 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
      " 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1\n",
      " 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1\n",
      " 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ticker in tickers_list:\n",
    "    pred = dict_of_models[ticker].predict(df_to_dataset(dict_of_datasets[ticker][2], ticker))\n",
    "    print(tf.argmax(pred, axis = 1).numpy().T)\n",
    "    print('\\n\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(tf.argmax(pred, axis = 1).numpy().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=301683, shape=(1968,), dtype=int64, numpy=array([0, 0, 0, ..., 0, 0, 0], dtype=int64)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predixtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
